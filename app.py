# app.py

import streamlit as st
from datetime import datetime, timedelta
import time
import re
import os
import json
import pandas as pd
from dotenv import load_dotenv
from io import BytesIO

# --- ëª¨ë“ˆ ì„í¬íŠ¸ ---
from modules import ai_service
from modules import database_manager
from modules import news_crawler
from modules import trend_analyzer
from modules import data_exporter


# --- Streamlit ì•± ì‹œì‘ ---
st.set_page_config(layout="wide", page_title="ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°")

st.title("ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.markdown("ì›í•˜ëŠ” í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ê°ì§€í•˜ê³ , AIê°€ ìš”ì•½í•œ ê¸°ì‚¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

# --- Potens.dev AI API í‚¤ ì„¤ì • ---
load_dotenv() # .env íŒŒì¼ ë¡œë“œ
POTENS_API_KEY = os.getenv("POTENS_API_KEY")

if not POTENS_API_KEY:
    st.error("ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— 'POTENS_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Potens.dev AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop() # API í‚¤ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
database_manager.init_db()

# --- Streamlit Session State ì´ˆê¸°í™” (ì•±ì´ ì²˜ìŒ ë¡œë“œë  ë•Œë§Œ ì‹¤í–‰) ---
# ì„¸ì…˜ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ê°’ ì„¤ì •
if 'trending_keywords_data' not in st.session_state:
    st.session_state['trending_keywords_data'] = [] # ì „ì²´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ë‚´ë¶€ ë¶„ì„ìš©)
if 'displayed_keywords' not in st.session_state:
    st.session_state['displayed_keywords'] = [] # UIì— í‘œì‹œë  í•„í„°ë§ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
if 'final_collected_articles' not in st.session_state:
    st.session_state['final_collected_articles'] = [] # AI ìš”ì•½ëœ ìµœì¢… ê¸°ì‚¬ ëª©ë¡
if 'ai_insights_summary' not in st.session_state: # AIê°€ í•©ì³ì„œ ìƒì„±í•œ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (íŠ¸ë Œë“œ ë³´ê³ ì„œ)
    st.session_state['ai_insights_summary'] = ""
if 'ai_trend_summary' not in st.session_state: # AIê°€ ìƒì„±í•œ íŠ¸ë Œë“œ ìš”ì•½ (ë¶„ë¦¬ëœ í˜¸ì¶œ)
    st.session_state['ai_trend_summary'] = ""
if 'ai_insurance_info' not in st.session_state: # AIê°€ ìƒì„±í•œ ë³´í—˜ ê´€ë ¨ ì •ë³´ (ë³µì›)
    st.session_state['ai_insurance_info'] = ""

# submitted_flagëŠ” í¼ ì œì¶œ ì‹œì—ë§Œ Trueê°€ ë˜ë„ë¡ ìœ ì§€
if 'submitted_flag' not in st.session_state:
    st.session_state['submitted_flag'] = False
# analysis_completed í”Œë˜ê·¸ëŠ” ë¶„ì„ ì™„ë£Œ ì‹œ True
if 'analysis_completed' not in st.session_state:
    st.session_state['analysis_completed'] = False
# DB ì´ˆê¸°í™” í›„ í‘œì‹œë  ë©”ì‹œì§€
if 'db_status_message' not in st.session_state:
    st.session_state['db_status_message'] = ""
if 'db_status_type' not in st.session_state:
    st.session_state['db_status_type'] = ""


# --- UI ë ˆì´ì•„ì›ƒ: ê²€ìƒ‰ ì¡°ê±´ (ì¢Œ) & í‚¤ì›Œë“œ íŠ¸ë Œë“œ ê²°ê³¼ (ìš°) ---
col_search_input, col_trend_results = st.columns([1, 2]) # 1:2 ë¹„ìœ¨ë¡œ ì»¬ëŸ¼ ë¶„í• 

with col_search_input:
    st.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •")
    with st.form("search_form"):
        keyword = st.text_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ í‚¤ì›Œë“œ (ì˜ˆ: 'ì „ê¸°ì°¨')", value="ì „ê¸°ì°¨", key="keyword_input")
        total_search_days = st.number_input("ì´ ëª‡ ì¼ê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ê¹Œìš”? (ì˜ˆ: 15)", min_value=1, value=15, key="total_days_input")
        recent_trend_days = st.number_input("ìµœê·¼ ëª‡ ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í• ê¹Œìš”? (ì˜ˆ: 2)", min_value=1, value=2, key="recent_days_input")
        max_naver_search_pages_per_day = st.number_input("ê° ë‚ ì§œë³„ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ëª‡ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§í• ê¹Œìš”? (í˜ì´ì§€ë‹¹ 10ê°œ ê¸°ì‚¬, ì˜ˆ: 3)", min_value=1, value=3, key="max_pages_input")

        # í¼ ì œì¶œ ë²„íŠ¼
        submitted = st.form_submit_button("ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")

with col_trend_results:
    st.header("ğŸ“ˆ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼")
    st.markdown("ë‹¤ìŒì€ ìµœê·¼ ì–¸ê¸‰ëŸ‰ì´ ê¸‰ì¦í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")

    # ì´ ì»¨í…Œì´ë„ˆëŠ” ë¶„ì„ ì§„í–‰ ìƒí™© ë©”ì‹œì§€ë‚˜ ì´ˆê¸° ë©”ì‹œì§€, ìµœì¢… ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    # í‘œì™€ ë©”ì‹œì§€ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬
    table_placeholder = st.empty() # í‘œë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
    status_message_placeholder = st.empty() # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ

    # --- ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ (submitted ë²„íŠ¼ í´ë¦­ ì‹œ) ---
    if submitted:
        # ìƒˆë¡œìš´ ê²€ìƒ‰ ìš”ì²­ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state['trending_keywords_data'] = []
        st.session_state['displayed_keywords'] = []
        st.session_state['final_collected_articles'] = []
        st.session_state['ai_insights_summary'] = ""
        st.session_state['ai_trend_summary'] = ""
        st.session_state['ai_insurance_info'] = "" # ì´ˆê¸°í™” ë³µì›

        st.session_state['submitted_flag'] = True
        st.session_state['analysis_completed'] = False
        st.session_state['db_status_message'] = ""
        st.session_state['db_status_type'] = ""

        table_placeholder.empty()
        my_bar = status_message_placeholder.progress(0, text="ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì§„í–‰ ì¤‘...")
        status_message_placeholder.info("ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        if recent_trend_days >= total_search_days:
            status_message_placeholder.error("ì˜¤ë¥˜: ìµœê·¼ íŠ¸ë Œë“œ ë¶„ì„ ê¸°ê°„ì€ ì´ ê²€ìƒ‰ ê¸°ê°„ë³´ë‹¤ ì§§ì•„ì•¼ í•©ë‹ˆë‹¤.")
        else:
            all_collected_news_metadata = []

            today_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            search_start_date = today_date - timedelta(days=total_search_days - 1)

            total_expected_articles = total_search_days * max_naver_search_pages_per_day * 10
            processed_article_count = 0


            for i in range(total_search_days):
                current_search_date = search_start_date + timedelta(days=i)
                formatted_search_date = current_search_date.strftime('%Y.%m.%d')

                daily_articles = news_crawler.crawl_naver_news_metadata(
                    keyword,
                    current_search_date,
                    max_naver_search_pages_per_day
                )

                for article in daily_articles:
                    processed_article_count += 1
                    progress_percentage = processed_article_count / total_expected_articles
                    my_bar.progress(min(progress_percentage, 1.0), text=f"ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({formatted_search_date}, {processed_article_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ)")


                    article_data_for_db = {
                        "ì œëª©": article["ì œëª©"],
                        "ë§í¬": article["ë§í¬"],
                        "ë‚ ì§œ": article["ë‚ ì§œ"].strftime('%Y-%m-%d'),
                        "ë‚´ìš©": article["ë‚´ìš©"]
                    }
                    database_manager.insert_article(article_data_for_db)

                    all_collected_news_metadata.append(article)

            my_bar.empty()
            status_message_placeholder.success(f"ì´ {len(all_collected_news_metadata)}ê°œì˜ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

            # --- 2. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ---
            status_message_placeholder.info("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
            with st.spinner("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                trending_keywords_data = trend_analyzer.analyze_keyword_trends(
                    all_collected_news_metadata,
                    recent_days_period=recent_trend_days,
                    total_days_period=total_search_days
                )
            st.session_state['trending_keywords_data'] = trending_keywords_data

            if trending_keywords_data:
                # --- AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ì„ ë³„ ---
                relevant_keywords_from_ai_raw = []
                with st.spinner("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„ ì¤‘..."):
                    relevant_keywords_from_ai_raw = ai_service.get_relevant_keywords(
                        trending_keywords_data,
                        "ì°¨ëŸ‰ë³´í—˜ì‚¬ì˜ ë³´í—˜ê°œë°œì",
                        POTENS_API_KEY
                    )

                filtered_trending_keywords = []
                if relevant_keywords_from_ai_raw:
                    filtered_trending_keywords = [
                        kw_data for kw_data in trending_keywords_data
                        if kw_data['keyword'] in relevant_keywords_from_ai_raw
                    ]
                    filtered_trending_keywords = sorted(filtered_trending_keywords, key=lambda x: x['recent_freq'], reverse=True)

                    status_message_placeholder.info(f"AIê°€ ì„ ë³„í•œ ë³´í—˜ ê°œë°œì ê´€ì ì˜ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ({len(filtered_trending_keywords)}ê°œ): {[kw['keyword'] for kw in filtered_trending_keywords]}")
                else:
                    status_message_placeholder.warning("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                    filtered_trending_keywords = trending_keywords_data

                top_3_relevant_keywords = filtered_trending_keywords[:3]
                st.session_state['displayed_keywords'] = top_3_relevant_keywords

                if top_3_relevant_keywords:
                    pass
                else:
                    status_message_placeholder.info("ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


                # --- 3. íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ (Potens.dev AI í™œìš©) ---
                status_message_placeholder.info("íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ ì¤‘ (Potens.dev AI í˜¸ì¶œ)...")

                recent_trending_articles_candidates = [
                    article for article in all_collected_news_metadata
                    if article.get("ë‚ ì§œ") and today_date - timedelta(days=recent_trend_days) <= article["ë‚ ì§œ"]
                ]

                processed_links = set()

                articles_for_ai_summary = []
                for article in recent_trending_articles_candidates:
                    text_for_trend_check = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "")
                    article_keywords_for_trend = trend_analyzer.extract_keywords_from_text(text_for_trend_check)

                    if any(trend_kw['keyword'] in article_keywords_for_trend for trend_kw in top_3_relevant_keywords):
                        articles_for_ai_summary.append(article)

                total_ai_articles_to_process = len(articles_for_ai_summary)

                if total_ai_articles_to_process == 0:
                    status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ìµœê·¼ ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    ai_progress_bar = st.progress(0, text=f"AIê°€ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ìš”ì•½ ì¤‘... (0/{total_ai_articles_to_process} ì™„ë£Œ)")
                    ai_processed_count = 0

                    temp_collected_articles = []
                    for article in articles_for_ai_summary:
                        if article["ë§í¬"] in processed_links:
                            continue

                        ai_processed_count += 1
                        ai_progress_bar.progress(ai_processed_count / total_ai_articles_to_process, text=f"AIê°€ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ìš”ì•½ ì¤‘... ({ai_processed_count}/{total_ai_articles_to_process} ì™„ë£Œ)")

                        article_date_str = article["ë‚ ì§œ"].strftime('%Y-%m-%d') if article["ë‚ ì§œ"] else 'N/A'

                        ai_processed_content = ai_service.get_article_summary(
                            article["ì œëª©"],
                            article["ë§í¬"],
                            article_date_str,
                            article["ë‚´ìš©"],
                            POTENS_API_KEY,
                            max_attempts=2
                        )

                        final_content = ""
                        if ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                           ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."):
                            final_content = f"ë³¸ë¬¸ ìš”ì•½ ì‹¤íŒ¨ (AI ì˜¤ë¥˜): {ai_processed_content}"
                            status_message_placeholder.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {final_content}")
                        else:
                            final_content = ai_service.clean_ai_response_text(ai_processed_content)

                        temp_collected_articles.append({
                            "ì œëª©": article["ì œëª©"],
                            "ë§í¬": article["ë§í¬"],
                            "ë‚ ì§œ": article_date_str,
                            "ë‚´ìš©": final_content
                        })
                        processed_links.add(article["ë§í¬"])
                        time.sleep(0.1)

                    ai_progress_bar.empty()
                    st.session_state['final_collected_articles'] = temp_collected_articles

                    if st.session_state['final_collected_articles']:
                        status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                        # --- 4. AIê°€ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (ë¶„ë¦¬ëœ í˜¸ì¶œ) ---
                        status_message_placeholder.info("AIê°€ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œ ì¤‘ (ë¶„ë¦¬ëœ í˜¸ì¶œ)...")

                        # AIì—ê²Œ ì „ë‹¬í•  ìš”ì•½ëœ ê¸°ì‚¬ ëª©ë¡ (ì „ì²´ ê¸°ì‚¬ ìš”ì•½ë³¸ì„ ì „ë‹¬)
                        articles_for_ai_insight_generation = st.session_state['final_collected_articles'] # ëª¨ë“  ìš”ì•½ ê¸°ì‚¬ ì „ë‹¬

                        # íŠ¸ë Œë“œ ìš”ì•½ í˜¸ì¶œ
                        with st.spinner("AIê°€ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ìš”ì•½ ì¤‘..."):
                            trend_summary = ai_service.get_overall_trend_summary(
                                articles_for_ai_insight_generation, # ì „ì²´ ìš”ì•½ ê¸°ì‚¬ ëª©ë¡ ì „ë‹¬
                                POTENS_API_KEY
                            )
                            st.session_state['ai_trend_summary'] = ai_service.clean_ai_response_text(trend_summary)
                            if st.session_state['ai_trend_summary'].startswith("ìš”ì•½ëœ ê¸°ì‚¬ê°€ ì—†ì–´") or \
                               st.session_state['ai_trend_summary'].startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                               st.session_state['ai_trend_summary'].startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."):
                                status_message_placeholder.error(f"AI íŠ¸ë Œë“œ ìš”ì•½ ì‹¤íŒ¨: {st.session_state['ai_trend_summary']}")
                            else:
                                status_message_placeholder.success("AI ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½ ì™„ë£Œ!")
                            time.sleep(1) # ë‹¤ìŒ AI í˜¸ì¶œ ì „ ì ì‹œ ëŒ€ê¸°

                        # ë³´í—˜ ê´€ë ¨ ì •ë³´ í˜¸ì¶œ (ì´ì œ íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì„ ì¸ìë¡œ ì „ë‹¬)
                        with st.spinner("AIê°€ ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘..."):
                            insurance_info = ai_service.get_insurance_implications_from_ai(
                                st.session_state['ai_trend_summary'], # ë³€ê²½ëœ ë¶€ë¶„: íŠ¸ë Œë“œ ìš”ì•½ë¬¸ ì „ë‹¬
                                POTENS_API_KEY
                            )
                            st.session_state['ai_insurance_info'] = ai_service.clean_ai_response_text(insurance_info)
                            if st.session_state['ai_insurance_info'].startswith("ìš”ì•½ëœ ê¸°ì‚¬ê°€ ì—†ì–´") or \
                               st.session_state['ai_insurance_info'].startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                               st.session_state['ai_insurance_info'].startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") or \
                               st.session_state['ai_insurance_info'].startswith("íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ì–´"): # íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ëŠ” ê²½ìš°ë„ ì‹¤íŒ¨
                                status_message_placeholder.error(f"AI ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ ë¶„ì„ ì‹¤íŒ¨: {st.session_state['ai_insurance_info']}")
                            else:
                                status_message_placeholder.success("AI ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ ë¶„ì„ ì™„ë£Œ!")
                            time.sleep(1) # ë‹¤ìŒ UI ì—…ë°ì´íŠ¸ ì „ ì ì‹œ ëŒ€ê¸°

                        # ë‘ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±
                        final_insights_text = ""
                        if st.session_state['ai_trend_summary'] and \
                           not st.session_state['ai_trend_summary'].startswith("AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨"):
                            final_insights_text += "### ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½\n"
                            final_insights_text += st.session_state['ai_trend_summary'] + "\n\n"
                        else:
                            final_insights_text += "### ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½ (ìƒì„± ì‹¤íŒ¨)\n"
                            final_insights_text += st.session_state['ai_trend_summary'] + "\n\n"

                        if st.session_state['ai_insurance_info'] and \
                           not st.session_state['ai_insurance_info'].startswith("AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") and \
                           not st.session_state['ai_insurance_info'].startswith("íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ì–´"): # íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ëŠ” ê²½ìš°ë„ ì‹¤íŒ¨
                            final_insights_text += "### ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì£¼ìš” ì‚¬ì‹¤ ë° ë²•ì  ì±…ì„\n"
                            final_insights_text += st.session_state['ai_insurance_info'] + "\n"
                        else:
                            final_insights_text += "### ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì£¼ìš” ì‚¬ì‹¤ ë° ë²•ì  ì±…ì„ (ìƒì„± ì‹¤íŒ¨)\n"
                            final_insights_text += st.session_state['ai_insurance_info'] + "\n"

                        # --- ë¶€ë¡ ì„¹ì…˜ ì¶”ê°€ ---
                        final_insights_text += "\n---\n\n"
                        final_insights_text += "## ë¶€ë¡\n\n"

                        # í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±° ì¶”ê°€
                        final_insights_text += "### í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±°\n"
                        if st.session_state['displayed_keywords']:
                            for kw_data in st.session_state['displayed_keywords']:
                                surge_ratio_display = f"{kw_data['surge_ratio']:.2f}x" if kw_data['surge_ratio'] != float('inf') else "ìƒˆë¡œìš´ íŠ¸ë Œë“œ"
                                final_insights_text += (
                                    f"- **í‚¤ì›Œë“œ**: {kw_data['keyword']}\n"
                                    f"  - ìµœê·¼ ì–¸ê¸‰ëŸ‰: {kw_data['recent_freq']}íšŒ\n"
                                    f"  - ì´ì „ ì–¸ê¸‰ëŸ‰: {kw_data['past_freq']}íšŒ\n"
                                    f"  - ì¦ê°€ìœ¨: {surge_ratio_display}\n"
                                )
                        else:
                            final_insights_text += "í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                        final_insights_text += "\n"

                        # ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                        if st.session_state['final_collected_articles']:
                            final_insights_text += "### ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸\n"
                            # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ 3ê°œì”© ë¬¶ì–´ ìš”ì•½í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì—¬ê¸°ì— ì ìš©
                            # ì „ì²´ ê¸°ì‚¬ ìš”ì•½ë³¸ì„ 3ê°œì”© ë¬¶ì–´ ë‹¤ì‹œ ìš”ì•½í•˜ëŠ” ê³¼ì •
                            articles_to_summarize_in_batches = st.session_state['final_collected_articles']
                            batch_size = 3 # 3ê°œì”© ë¬¶ì–´ì„œ ìš”ì•½
                            
                            processed_articles_for_appendix = []
                            for i in range(0, len(articles_to_summarize_in_batches), batch_size):
                                batch = articles_to_summarize_in_batches[i:i+batch_size]
                                combined_batch_summaries = "\n\n".join([
                                    f"ì œëª©: {art['ì œëª©']}\nìš”ì•½: {art['ë‚´ìš©']}" for art in batch
                                ])
                                
                                # ê° ë°°ì¹˜ ìš”ì•½ (AI í˜¸ì¶œ)
                                prompt_batch_summary = (
                                    f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ë“¤ì„ ê°„ê²°í•˜ê²Œ ì¢…í•© ìš”ì•½í•´ ì£¼ì„¸ìš”. (ìµœëŒ€ 150ì)\n\n"
                                    f"ê¸°ì‚¬ ìš”ì•½ ë°°ì¹˜:\n{combined_batch_summaries}"
                                )
                                response_batch_summary = ai_service.retry_ai_call(
                                    prompt_batch_summary, 
                                    POTENS_API_KEY, 
                                    max_retries=2, 
                                    delay_seconds=10
                                )
                                
                                if "text" in response_batch_summary:
                                    processed_articles_for_appendix.append(ai_service.clean_ai_response_text(response_batch_summary["text"]))
                                else:
                                    processed_articles_for_appendix.append(f"[ë°°ì¹˜ ìš”ì•½ ì‹¤íŒ¨: {response_batch_summary.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}]")
                                
                                time.sleep(0.5) # ë°°ì¹˜ ìš”ì•½ ê°„ ì§§ì€ ì§€ì—°
                            
                            # ìµœì¢…ì ìœ¼ë¡œ ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìš”ì•½ëœ ë°°ì¹˜ í˜•íƒœë¡œ ì¶”ê°€
                            for i, batch_summary in enumerate(processed_articles_for_appendix):
                                final_insights_text += (
                                    f"**ë°°ì¹˜ {i+1} ìš”ì•½**: {batch_summary}\n"
                                )
                            final_insights_text += "\n"
                        else:
                            final_insights_text += "ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

                        st.session_state['ai_insights_summary'] = final_insights_text

                    else:
                        status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            else:
                status_message_placeholder.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

        st.session_state['submitted_flag'] = False
        st.session_state['analysis_completed'] = True

    # --- ê²°ê³¼ê°€ ì´ë¯¸ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ” ê²½ìš° í‘œì‹œ ---
    if not st.session_state.get('submitted_flag', False) and \
       st.session_state.get('analysis_completed', False):
        if st.session_state['displayed_keywords']:
            df_top_keywords = pd.DataFrame(st.session_state['displayed_keywords'])
            df_top_keywords['surge_ratio'] = df_top_keywords['surge_ratio'].apply(
                lambda x: f"{x:.2f}x" if x != float('inf') else "ìƒˆë¡œìš´ íŠ¸ë Œë“œ"
            )
            table_placeholder.table(df_top_keywords)

            if st.session_state['final_collected_articles']:
                status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                # AI ì¸ì‚¬ì´íŠ¸ ìš”ì•½ í‘œì‹œ (íŠ¸ë Œë“œ ë³´ê³ ì„œ)
                if st.session_state['ai_insights_summary']:
                    st.markdown("---")
                    st.subheader("ğŸ’¡ AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸") # ì„¹ì…˜ëª… ë³µì›
                    st.markdown(st.session_state['ai_insights_summary'])
                else:
                    st.info("AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.") # ë©”ì‹œì§€ ë³µì›

        else:
            st.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # --- ì´ˆê¸° ë¡œë“œ ì‹œ ë©”ì‹œì§€ ---
    elif not st.session_state.get('submitted_flag', False) and \
         not st.session_state.get('analysis_completed', False):
        empty_df = pd.DataFrame(columns=['keyword', 'recent_freq', 'past_freq', 'surge_ratio'])
        table_placeholder.table(empty_df)
        status_message_placeholder.info("ê²€ìƒ‰ ì¡°ê±´ì„ ì…ë ¥í•˜ê³  'ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")


# --- ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
st.header("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
all_db_articles = database_manager.get_all_articles()

if all_db_articles:
    df_all_articles = pd.DataFrame(all_db_articles, columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©', 'ìˆ˜ì§‘_ì‹œê°„'])
    df_all_articles['ë‚´ìš©'] = df_all_articles['ë‚´ìš©'].fillna('')

    txt_data_all_crawled = data_exporter.export_articles_to_txt(
        [dict(zip(df_all_articles.columns, row)) for row in df_all_articles.values],
        file_prefix="all_crawled_news"
    )

    excel_data_all_crawled = data_exporter.export_articles_to_excel(df_all_articles, sheet_name='All_Crawled_News')


    df_ai_summaries = pd.DataFrame(st.session_state['final_collected_articles'],
                                   columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©'])
    df_ai_summaries['ë‚´ìš©'] = df_ai_summaries['ë‚´ìš©'].fillna('')

    txt_data_ai_summaries = data_exporter.export_articles_to_txt(
        [dict(zip(df_ai_summaries.columns, row)) for row in df_ai_summaries.values],
        file_prefix="ai_summaries"
    )

    excel_data_ai_summaries = None
    if not df_ai_summaries.empty:
        excel_data_ai_summaries = data_exporter.export_articles_to_excel(df_ai_summaries, sheet_name='AI_Summaries')

    txt_data_ai_insights = st.session_state['ai_insights_summary']


    st.markdown("### ğŸ“Š ìˆ˜ì§‘ëœ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°")
    col_all_data_txt, col_all_data_excel = st.columns([0.1, 0.9])
    with col_all_data_txt:
        st.download_button(
            label="ğŸ“„ TXT ë‹¤ìš´ë¡œë“œ",
            data=txt_data_all_crawled,
            file_name=data_exporter.generate_filename("all_crawled_news", "txt"),
            mime="text/plain",
            help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
        )
    with col_all_data_excel:
        st.download_button(
            label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_data_all_crawled.getvalue(),
            file_name=data_exporter.generate_filename("all_crawled_news", "xlsx"),
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (í•œê¸€ ê¹¨ì§ ì—†ìŒ)"
        )

    if not df_ai_summaries.empty:
        st.markdown("### ğŸ“ AI ìš”ì•½ ê¸°ì‚¬")
        col_ai_txt, col_ai_excel = st.columns([0.1, 0.9])
        with col_ai_txt:
            st.download_button(
                label="ğŸ“„ AI ìš”ì•½ TXT ë‹¤ìš´ë¡œë“œ",
                data=txt_data_ai_summaries,
                file_name=data_exporter.generate_filename("ai_summaries", "txt"),
                mime="text/plain",
                help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            )
        with col_ai_excel:
            st.download_button(
                label="ğŸ“Š AI ìš”ì•½ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=excel_data_ai_summaries.getvalue(),
                file_name=data_exporter.generate_filename("ai_summaries", "xlsx"),
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            )
    else:
        st.info("AI ìš”ì•½ëœ íŠ¸ë Œë“œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ ìš”ì•½ëœ ê¸°ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

    if st.session_state['ai_insights_summary']:
        st.markdown("### ğŸ’¡ AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸") # ì„¹ì…˜ëª… ë³µì›
        col_ai_insights_txt, _ = st.columns([0.1, 0.9])
        with col_ai_insights_txt:
            st.download_button(
                label="ğŸ“„ AI ì¸ì‚¬ì´íŠ¸ TXT ë‹¤ìš´ë¡œë“œ", # ë²„íŠ¼ í…ìŠ¤íŠ¸ ë³µì›
                data=txt_data_ai_insights,
                file_name=data_exporter.generate_filename("ai_insights", "txt"), # íŒŒì¼ëª… ë³µì›
                mime="text/plain",
                help="AIê°€ ë„ì¶œí•œ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤." # ë„ì›€ë§ ë³µì›
            )
    else:
        st.info("AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”.") # ë©”ì‹œì§€ ë³µì›


    st.markdown("---")
    col_db_info, col_db_clear = st.columns([2, 1])
    with col_db_info:
        st.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {len(all_db_articles)}ê°œì˜ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        if st.session_state['db_status_message']:
            if st.session_state['db_status_type'] == "success":
                st.success(st.session_state['db_status_message'])
            elif st.session_state['db_status_type'] == "error":
                st.error(st.session_state['db_status_message'])
            st.session_state['db_status_message'] = ""
            st.session_state['db_status_type'] = ""
        st.markdown("ğŸ’¡ **CSV íŒŒì¼ì´ ì—‘ì…€ì—ì„œ ê¹¨ì§ˆ ê²½ìš°:** ì—‘ì…€ì—ì„œ 'ë°ì´í„°' íƒ­ -> 'í…ìŠ¤íŠ¸/CSV ê°€ì ¸ì˜¤ê¸°'ë¥¼ í´ë¦­í•œ í›„, 'ì›ë³¸ íŒŒì¼' ì¸ì½”ë”©ì„ 'UTF-8'ë¡œ ì„ íƒí•˜ì—¬ ê°€ì ¸ì˜¤ì„¸ìš”.")
    with col_db_clear:
        if st.button("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”", help="ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì €ì¥ëœ ë‰´ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.", type="secondary"):
            database_manager.clear_db_content()
            st.session_state['trending_keywords_data'] = []
            st.session_state['displayed_keywords'] = []
            st.session_state['final_collected_articles'] = []
            st.session_state['ai_insights_summary'] = ""
            st.session_state['ai_trend_summary'] = ""
            st.session_state['ai_insurance_info'] = "" # ì´ˆê¸°í™” ë³µì›
            st.session_state['submitted_flag'] = False
            st.session_state['analysis_completed'] = False
            st.rerun()


# --- ì°¨ëŸ‰ ë³´í—˜ ì‹œì¥ ì¡°ì‚¬ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ (ì œê±°) ---
# ì´ ì„¹ì…˜ì€ ì´ì œ ì™„ì „íˆ ì œê±°ë©ë‹ˆë‹¤.
