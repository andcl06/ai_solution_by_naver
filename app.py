import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import csv
import re
import os
import json
import sqlite3 # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
import pandas as pd # CSV, Excel ìƒì„±ì„ ìœ„í•´ ì¶”ê°€
from dotenv import load_dotenv
from collections import Counter
from io import BytesIO # Excel íŒŒì¼ ìƒì„±ì„ ìœ„í•´ ì¶”ê°€

# --- Potens.dev AI API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ ---

def call_potens_api_raw(prompt_message: str, api_key: str, response_schema=None) -> dict:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ë¡œ Potens.dev APIë¥¼ í˜¸ì¶œí•˜ê³  ì›ë³¸ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    response_schema: JSON ì‘ë‹µì„ ìœ„í•œ ìŠ¤í‚¤ë§ˆ (ì„ íƒ ì‚¬í•­)
    """
    if not api_key:
        st.error("ğŸš¨ ì˜¤ë¥˜: Potens.dev API í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {"error": "Potens.dev API í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."}

    potens_api_endpoint = "https://ai.potens.ai/api/chat" 
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "prompt": prompt_message
    }
    if response_schema:
        payload["generationConfig"] = {
            "responseMimeType": "application/json",
            "responseSchema": response_schema
        }

    try:
        response = requests.post(potens_api_endpoint, headers=headers, json=payload, timeout=300)
        response.raise_for_status()
        response_json = response.json()
        
        if "message" in response_json:
            # response_schemaê°€ ìˆì„ ê²½ìš°, message í•„ë“œì˜ ë‚´ìš©ì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            if response_schema:
                try:
                    parsed_content = json.loads(response_json["message"].strip())
                    return {"text": parsed_content, "raw_response": response_json}
                except json.JSONDecodeError:
                    return {"error": f"Potens.dev API ì‘ë‹µ JSON ë””ì½”ë”© ì˜¤ë¥˜ (message í•„ë“œ): {response_json['message']}"}
            else:
                return {"text": response_json["message"].strip(), "raw_response": response_json}
        else:
            return {"error": "Potens.dev API ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¼ì§€ ì•ŠìŠµë‹ˆë‹¤.", "raw_response": response_json}

    except requests.exceptions.RequestException as e:
        error_message = f"Potens.dev API í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ ( network/timeout/HTTP): {e}"
        if e.response is not None:
            error_message += f" Response content: {e.response.text}" 
        return {"error": error_message}
    except json.JSONDecodeError:
        return {"error": f"Potens.dev API ì‘ë‹µ JSON ë””ì½”ë”© ì˜¤ë¥˜: {response.text}"}
    except Exception as e:
        return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}"}

def call_potens_ai_for_article_summary_with_context_single_call(title: str, link: str, date_str: str, summary_snippet: str, api_key: str, max_attempts: int = 2, delay_seconds: int = 15) -> str:
    """
    Potens.dev AIë¥¼ í˜¸ì¶œí•˜ì—¬ ì œê³µëœ ì œëª©, ë§í¬, ë‚ ì§œ, ë¯¸ë¦¬ë³´ê¸° ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ
    ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤. (ë‹¨ì¼ í˜¸ì¶œ)
    ë§í¬ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•  ê²½ìš°ì—ë„ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    initial_prompt = (
        f"ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n"
        f"**ì œê³µëœ ë§í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ê±°ë‚˜ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì•„ë˜ ì œê³µëœ ì œëª©, ë‚ ì§œ, ë¯¸ë¦¬ë³´ê¸° ìš”ì•½ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë‚´ìš©ì„ íŒŒì•…í•˜ê³  ìš”ì•½í•´ ì£¼ì„¸ìš”.**\n"
        f"ê´‘ê³ ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ì´ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì œê³µí•´ ì£¼ì„¸ìš”.\n\n"
        f"ì œëª©: {title}\n"
        f"ë§í¬: {link}\n"
        f"ë‚ ì§œ: {date_str}\n"
        f"ë¯¸ë¦¬ë³´ê¸° ìš”ì•½: {summary_snippet}"
    )
    
    # st.write(f"  - Potens.dev AI í˜¸ì¶œ (ê¸°ì‚¬ ìš”ì•½ ìš”ì²­)...") # UI ë¡œê·¸ ì œê±°
    for attempt in range(max_attempts):
        response_dict = call_potens_api_raw(initial_prompt, api_key=api_key)
        if "text" in response_dict:
            # st.write(f"    -> ì‹œë„ {attempt + 1} ì„±ê³µ.") # UI ë¡œê·¸ ì œê±°
            return response_dict["text"]
        else:
            error_msg = response_dict.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            # st.write(f"    -> ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {error_msg}. ì¬ì‹œë„í•©ë‹ˆë‹¤...") # UI ë¡œê·¸ ì œê±°
            if attempt < max_attempts - 1:
                time.sleep(delay_seconds)
            else:
                return f"Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {error_msg}"
    
    return "Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

def get_relevant_keywords_from_ai(trending_keywords_data: list[dict], perspective: str, api_key: str, max_attempts: int = 2, delay_seconds: int = 15) -> list[str]:
    """
    Potens.dev AIë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¤‘ íŠ¹ì • ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
    ë°˜í™˜ ê°’: ['keyword1', 'keyword2', ...]
    """
    prompt_keywords = [{"keyword": k['keyword'], "recent_freq": k['recent_freq']} for k in trending_keywords_data]
    
    prompt = (
        f"ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì‹ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ëª©ë¡ì…ë‹ˆë‹¤. ì´ í‚¤ì›Œë“œë“¤ì„ '{perspective}'ì˜ ê´€ì ì—ì„œ "
        f"ê°€ì¥ ìœ ì˜ë¯¸í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ìˆœì„œëŒ€ë¡œ ìµœëŒ€ 5ê°œê¹Œì§€ ê³¨ë¼ JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”. "
        f"ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ë°°ì—´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ê° í‚¤ì›Œë“œëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"í‚¤ì›Œë“œ ëª©ë¡: {json.dumps(prompt_keywords, ensure_ascii=False)}"
    )

    response_schema = {
        "type": "ARRAY",
        "items": {"type": "STRING"}
    }

    # st.info(f"AIê°€ '{perspective}' ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„ ì¤‘...") # UI ë¡œê·¸ ì œê±°
    for attempt in range(max_attempts):
        response_dict = call_potens_api_raw(prompt, api_key, response_schema=response_schema)
        if "text" in response_dict and isinstance(response_dict["text"], list):
            # st.success(f"AI í‚¤ì›Œë“œ ì„ ë³„ ì„±ê³µ (ì‹œë„ {attempt + 1}).") # UI ë¡œê·¸ ì œê±°
            return response_dict["text"]
        else:
            error_msg = response_dict.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            # st.warning(f"AI í‚¤ì›Œë“œ ì„ ë³„ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {error_msg}. ì¬ì‹œë„í•©ë‹ˆë‹¤...") # UI ë¡œê·¸ ì œê±°
            if attempt < max_attempts - 1:
                time.sleep(delay_seconds)
            else:
                # st.error(f"AI í‚¤ì›Œë“œ ì„ ë³„ ìµœì¢… ì‹¤íŒ¨: {error_msg}") # UI ë¡œê·¸ ì œê±°
                return []
    return []

def clean_ai_response_text(text: str) -> str:
    """
    AI ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸, ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ,
    ê·¸ë¦¬ê³  AIê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„œë‘ ë¬¸êµ¬ë“¤ì„ ì œê±°í•˜ì—¬ í‰íƒ„í™”í•©ë‹ˆë‹¤.
    """
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ì˜ˆ: ```json ... ```)
    cleaned_text = re.sub(r'```(?:json|text)?\s*([\s\S]*?)\s*```', r'\1', text, flags=re.IGNORECASE)
    
    # ë§ˆí¬ë‹¤ìš´ í—¤ë”, ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸, ë³¼ë“œì²´/ì´íƒ¤ë¦­ì²´ ê¸°í˜¸ ë“± ì œê±°
    cleaned_text = re.sub(r'#|\*|-|\+', '', cleaned_text)
    
    # ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ëª©ë¡ ë§ˆì»¤ ì œê±° (ì˜ˆ: "1.", "2.", "3.")
    cleaned_text = re.sub(r'^\s*\d+\.\s*', '', cleaned_text, flags=re.MULTILINE)

    # AIê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„œë‘ ë¬¸êµ¬ ì œê±° (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ìœ ì—°í•˜ê²Œ ë§¤ì¹­)
    patterns_to_remove = [
        r'ì œê³µí•´ì£¼ì‹ \s*URLì˜\s*ë‰´ìŠ¤\s*ê¸°ì‚¬\s*ë‚´ìš©ì„\s*ìš”ì•½í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ì£¼ìš”\s*ë‚´ìš©[.:\s]*', 
        r'ì œê³µí•´ì£¼ì‹ \s*í…ìŠ¤íŠ¸ë¥¼\s*ìš”ì•½\s*í•˜ê² \s*ìŠµë‹ˆë‹¤[.:\s]*\s*ìš”ì•½[.:\s]*',
        r'ìš”ì•½í•´\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.:\s]*\s*ì£¼ìš”\s*ë‚´ìš©\s*ìš”ì•½[.:\s]*',
        r'ë‹¤ìŒ\s*í…ìŠ¤íŠ¸ì˜\s*ìš”ì•½ì…ë‹ˆë‹¤[.:\s]*',
        r'ì£¼ìš”\s*ë‚´ìš©ì„\s*ìš”ì•½\s*í•˜ë©´\s*ë‹¤ìŒê³¼\s*ê°™ìŠµë‹ˆë‹¤[.:\s]*',
        r'í•µì‹¬\s*ë‚´ìš©ì€\s*ë‹¤ìŒê³¼\s*ê°™ìŠµë‹ˆë‹¤[.:\s]*',
        r'ìš”ì•½í•˜ìë©´[.:\s]*',
        r'ì£¼ìš”\s*ìš”ì•½[.:\s]*',
        r'í…ìŠ¤íŠ¸ë¥¼\s*ìš”ì•½í•˜ë©´\s*ë‹¤ìŒê³¼\s*ê°™ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ì œê³µëœ\s*í…ìŠ¤íŠ¸ì—\s*ëŒ€í•œ\s*ìš”ì•½ì…ë‹ˆë‹¤[.:\s]*',
        r'ë‹¤ìŒì€\s*aiê°€\s*ë‚´ìš©ì„\s*ìš”ì•½í•œ\s*ê²ƒì…ë‹ˆë‹¤[.:\s]*',
        r'ë¨¼ì €\s*ìµœì‹ \s*ì •ë³´ê°€\s*í•„ìš”í•©ë‹ˆë‹¤[.:\s]*\s*í˜„ì¬\s*ììœ¨ì£¼í–‰ì°¨\s*ê¸°ìˆ ê³¼\s*ê´€ë ¨ëœ\s*ìµœì‹ \s*íŠ¸ë Œë“œë¥¼\s*í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤[.:\s]*',
        r'ai\s*ë‹µë³€[.:\s]*', 
        r'ai\s*ë¶„ì„[.:\s]*', 
        r'ë‹¤ìŒì€\s*ìš”ì²­í•˜ì‹ \s*ë§í¬ì˜\s*ë³¸ë¬¸\s*ë‚´ìš©ì…ë‹ˆë‹¤[.:\s]*', 
        r'ë‹¤ìŒì€\s*ì œê³µëœ\s*ë‰´ìŠ¤\s*ê¸°ì‚¬ì˜\s*í•µì‹¬\s*ë‚´ìš©ì…ë‹ˆë‹¤[.:\s]*', 
        r'ë‰´ìŠ¤\s*ê¸°ì‚¬\s*ì£¼ìš”\s*ë‚´ìš©\s*ìš”ì•½[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*ì œê³µí•´ì£¼ì‹ \s*URLì—ì„œ\s*ë‰´ìŠ¤\s*ê¸°ì‚¬ì˜\s*ì£¼ìš”\s*ë‚´ìš©ì„\s*ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ì•˜ìŠµë‹ˆë‹¤[.:\s]*\s*\(1/3\)\s*í•´ë‹¹\s*ë§í¬ì—ì„œ\s*ë‰´ìŠ¤\s*ê¸°ì‚¬ì˜\s*í•µì‹¬\s*ë‚´ìš©ì„\s*ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*ì œê³µí•´ì£¼ì‹ \s*ë§í¬ì—ì„œ\s*ê¸°ì‚¬\s*ë‚´ìš©ì„\s*ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*í•´ë‹¹\s*URLì—ì„œ\s*ë‰´ìŠ¤\s*ê¸°ì‚¬ì˜\s*ì£¼ìš”\s*ë‚´ìš©ì„\s*ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*URLì„\s*ê²€ìƒ‰í•˜ì—¬\s*ê¸°ì‚¬\s*ë‚´ìš©ì„\s*í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤[.:\s]*\s*ê²€ìƒ‰\s*ê²°ê³¼ë¥¼\s*ë°”íƒ•ìœ¼ë¡œ\s*ë‹¤ìŒê³¼\s*ê°™ì´\s*ê¸°ì‚¬ì˜\s*í•µì‹¬\s*ë‚´ìš©ë§Œ\s*ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*í•´ë‹¹\s*URLì—ì„œ\s*ê¸°ì‚¬\s*ë‚´ìš©ì„\s*í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤[.:\s]*\s*ê¸°ì‚¬ì˜\s*ì£¼ìš”\s*ë‚´ìš©ì„\s*ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤[.:\s]*', 
        r'ê²€ìƒ‰ì„\s*ì§„í–‰í• \s*URLì„\s*ì°¾ê³ \s*ìˆì–´ìš”[.:\s]*\s*\(1/3\)\s*ì›¹ì‚¬ì´íŠ¸ì˜\s*ë‚´ìš©ì„\s*í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤[.:\s]*\s*ê¸°ì‚¬ì˜\s*ì£¼ìš”\s*ë‚´ìš©ì„\s*ê´‘ê³ ë‚˜\s*ë¶ˆí•„ìš”í•œ\s*ì •ë³´\s*ì—†ì´\s*ì¶”ì¶œí•´\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.:\s]*', 
        r'ì´ìƒì…ë‹ˆë‹¤[.:\s]*', 
        r'ì´ìƒì…ë‹ˆë‹¤[.:\s]*\s*ê´‘ê³ ë‚˜\s*ë¶ˆí•„ìš”í•œ\s*ì •ë³´ëŠ”\s*ì œì™¸í•˜ê³ \s*ì£¼ìš”\s*ë‚´ìš©ë§Œ\s*ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤[.:\s]*', 
        r'ì´ê²ƒì´\s*ì œê³µí•´ì£¼ì‹ \s*YTN\s*ë‰´ìŠ¤\s*ë§í¬ì—ì„œ\s*ì¶”ì¶œí•œ\s*í•µì‹¬\s*ê¸°ì‚¬\s*ë‚´ìš©ì…ë‹ˆë‹¤[.:\s]*\s*ê´‘ê³ ë‚˜\s*ë¶ˆí•„ìš”í•œ\s*ì •ë³´ëŠ”\s*ì œì™¸í•˜ê³ \s*ê¸°ì‚¬ì˜\s*ì£¼ìš”\s*ë‚´ìš©ë§Œ\s*ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤[.:\s]*', 
        r'ìœ„\s*ë‚´ìš©ì€\s*ì œê³µí•´ì£¼ì‹ \s*URLì—ì„œ\s*ì¶”ì¶œí•œ\s*ê¸°ì‚¬ì˜\s*í•µì‹¬\s*ë‚´ìš©ì…ë‹ˆë‹¤[.:\s]*\s*ê´‘ê³ ë‚˜\s*ë¶ˆí•„ìš”í•œ\s*ì •ë³´ë¥¼\s*ì œê±°í•˜ê³ \s*ì£¼ìš”\s*ë‚´ìš©ë§Œ\s*ì •ë¦¬í–ˆìŠµë‹ˆë‹¤[.:\s]*', 
        r'ì œê³µí•´ì£¼ì‹ \s*ë§í¬\(https?://[^\s]+\)\s*ëŠ”\s*ì—°í•©ë‰´ìŠ¤ì˜\s*ì‚¬ì§„\s*ê¸°ì‚¬ë¡œ,\s*\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼ì—\s*ê²Œì‹œëœ\s*ë‚´ìš©ì…ë‹ˆë‹¤[.:\s]*\s*ê¸°ì‚¬\s*ì œëª©:\s*""[^""]+""\s*í•µì‹¬\s*ë‚´ìš©:[.:\s]*', 
    ]
    for pattern in patterns_to_remove:
        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)

    # ì—¬ëŸ¬ ê°œì˜ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    cleaned_text = re.sub(r'\n+', ' ', cleaned_text)
    # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ëŒ€ì²´ (ì¤„ë°”ê¿ˆ ëŒ€ì²´ í›„ì—ë„ ì¤‘ë³µ ê³µë°±ì´ ìƒê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

# --- í‚¤ì›Œë“œ ì¶”ì¶œ ë° íŠ¸ë Œë“œ ë¶„ì„ í•¨ìˆ˜ë“¤ ---

def get_keywords_from_text(text: str) -> list[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ê°„ë‹¨í•œ í† í°í™”, ì†Œë¬¸ì ë³€í™˜, ë¶ˆìš©ì–´ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë” ì •êµí•œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°(ê¼¬ê¼¬ë§ˆ, konlpy ë“±)ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
    tokens = text.lower().split()
    
    # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ëª©ë¡ (í™•ì¥ ê°€ëŠ¥)
    stopwords = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì™€", "ê³¼", "ë„", "ë§Œ", "ê³ ", "ì—", "ì˜", "í•œ", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ëŒ€í•œ", "í†µí•´", "ì´ë²ˆ", "ì§€ë‚œ", "ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìœ„í•´", "ìœ¼ë¡œ", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œë¶€í„°", "ê¹Œì§€", "ë¶€í„°", "ìœ¼ë¡œ", "í•˜ì—¬", "ì—ê²Œ", "ì²˜ëŸ¼", "ë§Œí¼", "ë“¯ì´", "ë³´ë‹¤", "ì•„ë‹ˆë¼", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ", "ë•Œë¬¸ì—", "ëŒ€í•´", "ê´€ë ¨", "ì§€ë‚œ", "ìµœê·¼", "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ì „", "ì˜¤í›„", "ì˜¤í›„", "ì˜¤ì „", "ê¸°ì", "ë‰´ìŠ¤", "ì—°í•©ë‰´ìŠ¤", "ì¡°ì„ ë¹„ì¦ˆ", "í•œê²¨ë ˆ", "YTN", "MBN", "ë‰´ì‹œìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ"]
    
    # ë‘ ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ í¬í•¨í•˜ê³  ë¶ˆìš©ì–´ ì œê±°
    keywords = [word for word in tokens if len(word) > 1 and word not in stopwords]
    return keywords

def analyze_keyword_trends(articles_metadata: list[dict], recent_days_period: int = 2, total_days_period: int = 15, min_surge_ratio: float = 1.5, min_recent_freq: int = 3) -> list[dict]:
    """
    ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    recent_days_period: íŠ¸ë Œë“œë¥¼ ê°ì§€í•  ìµœê·¼ ê¸°ê°„ (ì˜ˆ: 2ì¼)
    total_days_period: ë¹„êµí•  ì „ì²´ ê¸°ê°„ (ì˜ˆ: 15ì¼)
    min_surge_ratio: ìµœê·¼ ê¸°ê°„ ë¹ˆë„ / ê³¼ê±° ê¸°ê°„ ë¹ˆë„ ë¹„ìœ¨ì´ ì´ ê°’ ì´ìƒì¼ ë•Œ íŠ¸ë Œë“œë¡œ ê°„ì£¼
    min_recent_freq: ìµœê·¼ ê¸°ê°„ì— ìµœì†Œí•œ ì´ íšŸìˆ˜ ì´ìƒ ì–¸ê¸‰ë˜ì–´ì•¼ íŠ¸ë Œë“œë¡œ ê°„ì£¼
    ë°˜í™˜ ê°’: [{keyword: str, recent_freq: int, past_freq: int, surge_ratio: float}]
    """
    if not articles_metadata:
        return []

    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    recent_articles = []
    past_articles = []

    for article in articles_metadata:
        article_date = article.get("ë‚ ì§œ")
        if not isinstance(article_date, datetime):
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš°, ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°„ì£¼í•˜ì—¬ ì²˜ë¦¬ (ì •í™•ë„ ë‚®ìŒ)
            st.warning(f"ê²½ê³ : '{article['ì œëª©']}' ê¸°ì‚¬ì˜ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨. ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°„ì£¼í•˜ì—¬ ë¶„ì„ì— í¬í•¨í•©ë‹ˆë‹¤.")
            article_date = today

        if today - timedelta(days=recent_days_period) <= article_date:
            recent_articles.append(article)
        elif today - timedelta(days=total_days_period) <= article_date < today - timedelta(days=recent_days_period):
            past_articles.append(article)

    # ê° ê¸°ê°„ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    recent_keywords = Counter()
    for article in recent_articles:
        # íŠ¸ë Œë“œ ë¶„ì„ ì‹œ ì œëª©ê³¼ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí« ëª¨ë‘ í™œìš©
        text_for_keywords = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "") # 'ë‚´ìš©'ì´ ì´ì œ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí«
        recent_keywords.update(get_keywords_from_text(text_for_keywords))

    past_keywords = Counter()
    for article in past_articles:
        # íŠ¸ë Œë“œ ë¶„ì„ ì‹œ ì œëª©ê³¼ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí« ëª¨ë‘ í™œìš©
        text_for_keywords = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "") # 'ë‚´ìš©'ì´ ì´ì œ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí«
        past_keywords.update(get_keywords_from_text(text_for_keywords))

    trending_keywords_list = [] # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€ê²½
    for keyword, recent_freq in recent_keywords.items():
        past_freq = past_keywords.get(keyword, 0) # ê³¼ê±° ê¸°ê°„ì— ì—†ìœ¼ë©´ 0
        
        # ìµœê·¼ ê¸°ê°„ì— ìµœì†Œ ë¹ˆë„ ì´ìƒì´ì–´ì•¼ í•¨
        if recent_freq < min_recent_freq:
            continue

        surge_ratio = None
        if past_freq == 0:
            # ê³¼ê±°ì— ì—†ì—ˆëŠ”ë° ìµœê·¼ì— ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œëŠ” íŠ¸ë Œë“œë¡œ ê°„ì£¼
            if recent_freq >= min_recent_freq: 
                surge_ratio = float('inf') # ë¬´í•œëŒ€ë¡œ í‘œí˜„
        else:
            surge_ratio = recent_freq / past_freq
            if surge_ratio < min_surge_ratio: # ìµœì†Œ ì¦ê°€ìœ¨ ë¯¸ë‹¬ ì‹œ íŠ¸ë Œë“œ ì•„ë‹˜
                continue
        
        trending_keywords_list.append({
            "keyword": keyword,
            "recent_freq": recent_freq,
            "past_freq": past_freq,
            "surge_ratio": surge_ratio
        })
    
    # ë¹ˆë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    trending_keywords_list = sorted(trending_keywords_list, key=lambda x: x['recent_freq'], reverse=True)
    
    # st.info(f"--- í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ ---") # UI ë¡œê·¸ ì œê±°
    # st.info(f"  - ìµœê·¼ {recent_days_period}ì¼ê°„ ê¸°ì‚¬ ìˆ˜: {len(recent_articles)}") # UI ë¡œê·¸ ì œê±°
    # st.info(f"  - ê³¼ê±° {total_days_period - recent_days_period}ì¼ê°„ ê¸°ì‚¬ ìˆ˜: {len(past_articles)}") # UI ë¡œê·¸ ì œê±°
    # st.info(f"  - ì‹ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ({len(trending_keywords_list)}ê°œ): {[kw['keyword'] for kw in trending_keywords_list]}") # UI ë¡œê·¸ ì œê±°
    
    return trending_keywords_list

# --- SQLite ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ ---
DB_FILE = 'news_data.db'

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            link TEXT UNIQUE NOT NULL,
            date TEXT NOT NULL,
            content TEXT,
            crawl_timestamp TEXT NOT NULL
        )
    ''')
    conn.commit()
    conn.close()

def insert_article(article: dict):
    """ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…í•©ë‹ˆë‹¤. ì¤‘ë³µ ë§í¬ëŠ” ê±´ë„ˆë›°ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    try:
        # ë§í¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        c.execute("INSERT OR REPLACE INTO articles (link, title, date, content, crawl_timestamp) VALUES (?, ?, ?, ?, ?)",
                  (article['ë§í¬'], article['ì œëª©'], article['ë‚ ì§œ'], article['ë‚´ìš©'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        conn.commit()
    except Exception as e:
        print(f"ì˜¤ë¥˜: ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì…/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - {e} (ë§í¬: {article['ë§í¬']})")
    finally:
        conn.close()

def get_all_articles():
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT title, link, date, content, crawl_timestamp FROM articles ORDER BY date DESC, crawl_timestamp DESC")
    articles = c.fetchall()
    conn.close()
    return articles

def clear_db_content():
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ì‚¬ ê¸°ë¡ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    try:
        c.execute("DELETE FROM articles")
        conn.commit()
        # st.success("ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ë¡ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.") # ì´ ë©”ì‹œì§€ëŠ” ì„¸ì…˜ ìƒíƒœë¡œ ê´€ë¦¬
        st.session_state['db_status_message'] = "ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ë¡ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        st.session_state['db_status_type'] = "success"
    except Exception as e:
        # st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ì´ ë©”ì‹œì§€ë„ ì„¸ì…˜ ìƒíƒœë¡œ ê´€ë¦¬
        st.session_state['db_status_message'] = f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        st.session_state['db_status_type'] = "error"
    finally:
        conn.close()

# --- Streamlit ì•± ì‹œì‘ ---
st.set_page_config(layout="wide", page_title="ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°")

st.title("ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.markdown("ì›í•˜ëŠ” í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ê°ì§€í•˜ê³ , AIê°€ ìš”ì•½í•œ ê¸°ì‚¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

# --- Potens.dev AI API í‚¤ ì„¤ì • ---
load_dotenv() # .env íŒŒì¼ ë¡œë“œ
POTENS_API_KEY = os.getenv("POTENS_API_KEY")

if not POTENS_API_KEY:
    st.error("ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— 'POTENS_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Potens.dev AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop() # API í‚¤ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
init_db()

# --- Streamlit Session State ì´ˆê¸°í™” (ì•±ì´ ì²˜ìŒ ë¡œë“œë  ë•Œë§Œ ì‹¤í–‰) ---
# ì„¸ì…˜ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ê°’ ì„¤ì •
if 'trending_keywords_data' not in st.session_state:
    st.session_state['trending_keywords_data'] = [] # ì „ì²´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ë‚´ë¶€ ë¶„ì„ìš©)
if 'displayed_trending_keywords' not in st.session_state:
    st.session_state['displayed_trending_keywords'] = [] # UIì— í‘œì‹œë  í•„í„°ë§ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
if 'final_collected_articles' not in st.session_state:
    st.session_state['final_collected_articles'] = [] # AI ìš”ì•½ëœ ìµœì¢… ê¸°ì‚¬ ëª©ë¡
# submitted_flagëŠ” í¼ ì œì¶œ ì‹œì—ë§Œ Trueê°€ ë˜ë„ë¡ ìœ ì§€
if 'submitted_flag' not in st.session_state:
    st.session_state['submitted_flag'] = False
# analysis_completed í”Œë˜ê·¸ëŠ” ë¶„ì„ ì™„ë£Œ ì‹œ True
if 'analysis_completed' not in st.session_state:
    st.session_state['analysis_completed'] = False
# DB ì´ˆê¸°í™” í›„ í‘œì‹œë  ë©”ì‹œì§€
if 'db_status_message' not in st.session_state:
    st.session_state['db_status_message'] = ""
if 'db_status_type' not in st.session_state:
    st.session_state['db_status_type'] = ""


# --- UI ë ˆì´ì•„ì›ƒ: ê²€ìƒ‰ ì¡°ê±´ (ì¢Œ) & í‚¤ì›Œë“œ íŠ¸ë Œë“œ ê²°ê³¼ (ìš°) ---
col_search_input, col_trend_results = st.columns([1, 2]) # 1:2 ë¹„ìœ¨ë¡œ ì»¬ëŸ¼ ë¶„í• 

with col_search_input:
    st.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •")
    with st.form("search_form"):
        keyword = st.text_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ í‚¤ì›Œë“œ (ì˜ˆ: 'ì „ê¸°ì°¨')", value="ì „ê¸°ì°¨", key="keyword_input")
        total_search_days = st.number_input("ì´ ëª‡ ì¼ê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ê¹Œìš”? (ì˜ˆ: 15)", min_value=1, value=15, key="total_days_input")
        recent_trend_days = st.number_input("ìµœê·¼ ëª‡ ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í• ê¹Œìš”? (ì˜ˆ: 2)", min_value=1, value=2, key="recent_days_input")
        max_naver_search_pages_per_day = st.number_input("ê° ë‚ ì§œë³„ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ëª‡ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§í• ê¹Œìš”? (í˜ì´ì§€ë‹¹ 10ê°œ ê¸°ì‚¬, ì˜ˆ: 3)", min_value=1, value=3, key="max_pages_input")
        
        # í¼ ì œì¶œ ë²„íŠ¼
        submitted = st.form_submit_button("ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")

with col_trend_results:
    st.header("ğŸ“ˆ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼")
    st.markdown("ë‹¤ìŒì€ ìµœê·¼ ì–¸ê¸‰ëŸ‰ì´ ê¸‰ì¦í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
    
    # ì´ ì»¨í…Œì´ë„ˆëŠ” ë¶„ì„ ì§„í–‰ ìƒí™© ë©”ì‹œì§€ë‚˜ ì´ˆê¸° ë©”ì‹œì§€, ìµœì¢… ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    # í‘œì™€ ë©”ì‹œì§€ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬
    table_placeholder = st.empty() # í‘œë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
    status_message_placeholder = st.empty() # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ

    # --- ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ (submitted ë²„íŠ¼ í´ë¦­ ì‹œ) ---
    if submitted:
        # ìƒˆë¡œìš´ ê²€ìƒ‰ ìš”ì²­ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state['trending_keywords_data'] = []
        st.session_state['displayed_trending_keywords'] = []
        st.session_state['final_collected_articles'] = []
        st.session_state['submitted_flag'] = True # ì œì¶œ í”Œë˜ê·¸ ì„¤ì •
        st.session_state['analysis_completed'] = False # ë¶„ì„ ì™„ë£Œ í”Œë˜ê·¸ ì´ˆê¸°í™”
        st.session_state['db_status_message'] = "" # DB ì´ˆê¸°í™” ë©”ì‹œì§€ ì´ˆê¸°í™”
        st.session_state['db_status_type'] = "" # DB ì´ˆê¸°í™” ë©”ì‹œì§€ íƒ€ì… ì´ˆê¸°í™”
        
        # results_display_containerë¥¼ ë¹„ìš°ê³  ìƒˆë¡œìš´ ì§„í–‰ ìƒí™© í‘œì‹œ
        table_placeholder.empty() # í‘œ ì»¨í…Œì´ë„ˆ ë¹„ìš°ê¸°
        my_bar = status_message_placeholder.progress(0, text="ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì§„í–‰ ì¤‘...") # ì§„í–‰ë°”ì™€ ë©”ì‹œì§€ í‘œì‹œ
        status_message_placeholder.info("ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...") 

        if recent_trend_days >= total_search_days:
            status_message_placeholder.error("ì˜¤ë¥˜: ìµœê·¼ íŠ¸ë Œë“œ ë¶„ì„ ê¸°ê°„ì€ ì´ ê²€ìƒ‰ ê¸°ê°„ë³´ë‹¤ ì§§ì•„ì•¼ í•©ë‹ˆë‹¤.")
        else:
            all_collected_news_metadata = []
            
            # --- 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ---
            # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ ì‹œì‘ ë‚ ì§œ ê³„ì‚° (submitted ë¸”ë¡ ì•ˆì—ì„œ ì‚¬ìš©)
            today_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            search_start_date = today_date - timedelta(days=total_search_days - 1)

            total_iterations = total_search_days * max_naver_search_pages_per_day
            current_iteration = 0

            for i in range(total_search_days):
                current_search_date = search_start_date + timedelta(days=i)
                formatted_search_date = current_search_date.strftime('%Y.%m.%d')

                for page in range(max_naver_search_pages_per_day):
                    current_iteration += 1
                    my_bar.progress(current_iteration / total_iterations, text=f"ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({formatted_search_date}, {page+1}í˜ì´ì§€)")
                    
                    start_num = page * 10 + 1
                    search_url = (
                        f"https://search.naver.com/search.naver?where=news&query={keyword}"
                        f"&sm=tab_opt&sort=0&photo=0&field=0&pd=3"
                        f"&ds={formatted_search_date}"
                        f"&de={formatted_search_date}"
                        f"&start={start_num}"
                    )
                    
                    try:
                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0'}
                        response = requests.get(search_url, headers=headers)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, "html.parser")

                        title_spans = soup.find_all("span", class_="sds-comps-text-type-headline1")

                        if not title_spans:
                            break
                        else:
                            articles_on_this_page = 0
                            for title_span in title_spans:
                                link_tag = title_span.find_parent('a') 
                                
                                if link_tag and 'href' in link_tag.attrs:
                                    title = title_span.text.strip()
                                    link = link_tag['href']
                                    
                                    summary_snippet_text = ""
                                    next_sibling_a_tag = link_tag.find_next_sibling('a')
                                    if next_sibling_a_tag:
                                        snippet_span = next_sibling_a_tag.find('span', class_='sds-comps-text-type-body1')
                                        if snippet_span:
                                            summary_snippet_text = snippet_span.get_text(strip=True)
                                        else:
                                            summary_snippet_text = next_sibling_a_tag.get_text(strip=True)
                                    
                                    if not (link.startswith('javascript:') or 'ad.naver.com' in link):
                                        pub_date_obj = current_search_date
                                        
                                        article_data_for_db = { # DB ì €ì¥ì„ ìœ„í•œ ë°ì´í„°
                                            "ì œëª©": title,
                                            "ë§í¬": link,
                                            "ë‚ ì§œ": pub_date_obj.strftime('%Y-%m-%d'), # DB ì €ì¥ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ ë³€í™˜
                                            "ë‚´ìš©": summary_snippet_text if summary_snippet_text else "" # None ë°©ì§€
                                        }
                                        all_collected_news_metadata.append({ # íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°
                                            "ì œëª©": title,
                                            "ë§í¬": link,
                                            "ë‚ ì§œ": pub_date_obj, # datetime ê°ì²´ ìœ ì§€
                                            "ë‚´ìš©": summary_snippet_text
                                        })
                                        insert_article(article_data_for_db) # DBì— ì €ì¥
                                        articles_on_this_page += 1
                        
                        if articles_on_this_page == 0:
                            break
                        
                        time.sleep(0.5)

                    except requests.exceptions.RequestException as e:
                        status_message_placeholder.error(f"ì›¹ í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({formatted_search_date} ë‚ ì§œ, í˜ì´ì§€ {page + 1}): {e}")
                        break
                    except Exception as e:
                        status_message_placeholder.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({formatted_search_date} ë‚ ì§œ, í˜ì´ì§€ {page + 1}): {e}")
                        break
            my_bar.empty() # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìˆ¨ê¸°ê¸°
            status_message_placeholder.success(f"ì´ {len(all_collected_news_metadata)}ê°œì˜ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

            # --- 2. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ---
            status_message_placeholder.info("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
            with st.spinner("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."): # ìŠ¤í”¼ë„ˆëŠ” ìœ ì§€
                trending_keywords_data = analyze_keyword_trends(
                    all_collected_news_metadata, 
                    recent_days_period=recent_trend_days, 
                    total_days_period=total_search_days
                )
            st.session_state['trending_keywords_data'] = trending_keywords_data # ì„¸ì…˜ ìƒíƒœì— ì „ì²´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
            
            if trending_keywords_data:
                # status_message_placeholder.markdown("ë‹¤ìŒì€ ìµœê·¼ ì–¸ê¸‰ëŸ‰ì´ ê¸‰ì¦í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œì…ë‹ˆë‹¤.") # UIì— ì´ë¯¸ ì„¤ëª… ìˆìŒ
                
                # --- AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ì„ ë³„ ---
                relevant_keywords_from_ai_raw = []
                with st.spinner("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„ ì¤‘..."):
                    relevant_keywords_from_ai_raw = get_relevant_keywords_from_ai(
                        trending_keywords_data, 
                        "ì°¨ëŸ‰ë³´í—˜ì‚¬ì˜ ë³´í—˜ê°œë°œì", 
                        POTENS_API_KEY
                    )
                
                # AIê°€ ë°˜í™˜í•œ í‚¤ì›Œë“œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ trending_keywords_dataì™€ ë§¤ì¹­
                filtered_trending_keywords = []
                if relevant_keywords_from_ai_raw:
                    # AIê°€ ë°˜í™˜í•œ í‚¤ì›Œë“œë§Œ í•„í„°ë§í•˜ê³ , ì›ë˜ì˜ ë¹ˆë„ ë°ì´í„°ë¥¼ ìœ ì§€
                    filtered_trending_keywords = [
                        kw_data for kw_data in trending_keywords_data 
                        if kw_data['keyword'] in relevant_keywords_from_ai_raw
                    ]
                    # ë‹¤ì‹œ ë¹ˆë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (AIê°€ ìˆœì„œë¥¼ ì£¼ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
                    filtered_trending_keywords = sorted(filtered_trending_keywords, key=lambda x: x['recent_freq'], reverse=True)
                    
                    status_message_placeholder.info(f"AIê°€ ì„ ë³„í•œ ë³´í—˜ ê°œë°œì ê´€ì ì˜ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ({len(filtered_trending_keywords)}ê°œ): {[kw['keyword'] for kw in filtered_trending_keywords]}")
                else:
                    status_message_placeholder.warning("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                    filtered_trending_keywords = trending_keywords_data # AI ì„ ë³„ ì‹¤íŒ¨ ì‹œ ì „ì²´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì‚¬ìš©

                # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ìµœì¢… íŠ¸ë Œë“œë¡œ ì¸ì • (UIì— í‘œì‹œë  í‚¤ì›Œë“œ)
                top_3_relevant_keywords = filtered_trending_keywords[:3]
                st.session_state['displayed_trending_keywords'] = top_3_relevant_keywords # ì„¸ì…˜ ìƒíƒœì— UI í‘œì‹œìš© ì €ì¥

                if top_3_relevant_keywords:
                    # st.markdown(f"**ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ê°€ì¥ ìœ ì˜ë¯¸í•œ ìƒìœ„ {len(top_3_relevant_keywords)}ê°œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ:**") # UIì— ì´ë¯¸ ì„¤ëª… ìˆìŒ
                    # ê²°ê³¼ í…Œì´ë¸”ì€ ë‚˜ì¤‘ì— results_display_containerë¥¼ í†µí•´ ë‹¨ì¼ í‘œì‹œ
                    pass # í…Œì´ë¸” í‘œì‹œ ë¡œì§ì€ ìµœì¢… í‘œì‹œ ë¶€ë¶„ìœ¼ë¡œ ì´ë™
                else:
                    status_message_placeholder.info("ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


                # --- 3. íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ (Potens.dev AI í™œìš©) ---
                status_message_placeholder.info("íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ ì¤‘ (Potens.dev AI í˜¸ì¶œ)...")
                
                recent_trending_articles_candidates = [
                    article for article in all_collected_news_metadata
                    if article.get("ë‚ ì§œ") and today_date - timedelta(days=recent_trend_days) <= article["ë‚ ì§œ"]
                ]

                processed_links = set()
                
                # AI ìš”ì•½ ëŒ€ìƒ ê¸°ì‚¬ í•„í„°ë§ (ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ë§Œ)
                articles_for_ai_summary = []
                for article in recent_trending_articles_candidates:
                    text_for_trend_check = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "")
                    article_keywords_for_trend = get_keywords_from_text(text_for_trend_check)
                    
                    # ìƒìœ„ 3ê°œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ë§Œ ì„ íƒ
                    if any(trend_kw['keyword'] in article_keywords_for_trend for trend_kw in top_3_relevant_keywords):
                        articles_for_ai_summary.append(article)
                
                total_ai_articles_to_process = len(articles_for_ai_summary)

                if total_ai_articles_to_process == 0:
                    status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ìµœê·¼ ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    ai_progress_bar = st.progress(0, text=f"AIê°€ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ìš”ì•½ ì¤‘... (0/{total_ai_articles_to_process} ì™„ë£Œ)")
                    ai_processed_count = 0

                    temp_collected_articles = [] # AI ìš”ì•½ ê²°ê³¼ë¥¼ ì„ì‹œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                    for article in articles_for_ai_summary:
                        if article["ë§í¬"] in processed_links:
                            continue # ì¤‘ë³µ ë§í¬ ê±´ë„ˆë›°ê¸°

                        ai_processed_count += 1
                        ai_progress_bar.progress(ai_processed_count / total_ai_articles_to_process, text=f"AIê°€ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ìš”ì•½ ì¤‘... ({ai_processed_count}/{total_ai_articles_to_process} ì™„ë£Œ)")
                        
                        # st.markdown(f"**[íŠ¸ë Œë“œ ê¸°ì‚¬] {article['ì œëª©']}**") # ì›¹ UIì— ê¸°ì‚¬ ì œëª© í‘œì‹œ (ì´ì œ ìš”ì•½ì€ íŒŒì¼ë¡œë§Œ)
                        
                        article_date_str = article["ë‚ ì§œ"].strftime('%Y-%m-%d') if article["ë‚ ì§œ"] else 'N/A'

                        ai_processed_content = call_potens_ai_for_article_summary_with_context_single_call(
                            article["ì œëª©"], 
                            article["ë§í¬"], 
                            article_date_str, 
                            article["ë‚´ìš©"], # ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí«
                            POTENS_API_KEY, 
                            max_attempts=2
                        )
                        
                        final_content = ""
                        if ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                           ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."):
                            final_content = f"ë³¸ë¬¸ ìš”ì•½ ì‹¤íŒ¨ (AI ì˜¤ë¥˜): {ai_processed_content}"
                            status_message_placeholder.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {final_content}") 
                        else:
                            final_content = clean_ai_response_text(ai_processed_content)
                        
                        temp_collected_articles.append({ # ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                            "ì œëª©": article["ì œëª©"],
                            "ë§í¬": article["ë§í¬"],
                            "ë‚ ì§œ": article_date_str,
                            "ë‚´ìš©": final_content # AIê°€ ìš”ì•½í•œ ë‚´ìš©
                        })
                        processed_links.add(article["ë§í¬"])
                        time.sleep(0.1) # ê° ê¸°ì‚¬ ì²˜ë¦¬ ì‚¬ì´ì— ì§§ì€ ë”œë ˆì´

                    ai_progress_bar.empty() # AI ìš”ì•½ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìˆ¨ê¸°ê¸°
                    st.session_state['final_collected_articles'] = temp_collected_articles # ìµœì¢…ì ìœ¼ë¡œ ì„¸ì…˜ ìƒíƒœì— ì €ì¥

                    if st.session_state['final_collected_articles']: # ì„¸ì…˜ ìƒíƒœì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°
                        status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
                    else:
                        status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                
            else: # trending_keywords_dataê°€ ì—†ëŠ” ê²½ìš°
                status_message_placeholder.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ì‹ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # submitted í”Œë˜ê·¸ëŠ” ì œì¶œ í›„ ë‹¤ì‹œ Falseë¡œ ì„¤ì •í•˜ì—¬ ë‹¤ìŒ ë Œë”ë§ ì‹œ ìë™ ì‹¤í–‰ ë°©ì§€
        st.session_state['submitted_flag'] = False 
        st.session_state['analysis_completed'] = True # ë¶„ì„ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •

    # --- ê²°ê³¼ê°€ ì´ë¯¸ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ” ê²½ìš° í‘œì‹œ ---
    # submittedê°€ Falseì¼ ë•Œ (ì¦‰, ìƒˆë¡œê³ ì¹¨ë˜ê±°ë‚˜ ë‹¤ë¥¸ ìœ„ì ¯ì´ ë³€ê²½ë  ë•Œ)
    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ë°ì´í„°ê°€ ìˆê³ , ë¶„ì„ì´ ì™„ë£Œëœ ìƒíƒœë¼ë©´ í•´ë‹¹ ë°ì´í„°ë¥¼ results_display_containerì— í‘œì‹œ
    if not st.session_state.get('submitted_flag', False) and \
       st.session_state.get('analysis_completed', False): # ë¶„ì„ ì™„ë£Œ ìƒíƒœì¼ ë•Œë§Œ í‘œì‹œ
        if st.session_state['displayed_trending_keywords']: # í•„í„°ë§ëœ í‚¤ì›Œë“œ ì‚¬ìš©
            df_top_keywords = pd.DataFrame(st.session_state['displayed_trending_keywords'])
            df_top_keywords['surge_ratio'] = df_top_keywords['surge_ratio'].apply(
                lambda x: f"{x:.2f}x" if x != float('inf') else "ìƒˆë¡œìš´ íŠ¸ë Œë“œ"
            )
            table_placeholder.table(df_top_keywords) # ìµœì¢… ê²°ê³¼ë¥¼ table_placeholderì— í‘œì‹œ

            if st.session_state['final_collected_articles']:
                status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        else: # íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì—†ëŠ”ë° ë¶„ì„ì€ ì™„ë£Œëœ ê²½ìš°
            status_message_placeholder.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # --- ì´ˆê¸° ë¡œë“œ ì‹œ ë©”ì‹œì§€ (submitted_flagê°€ Falseì´ê³ , ì•„ì§ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°) ---
    elif not st.session_state.get('submitted_flag', False) and \
         not st.session_state.get('analysis_completed', False):
        # ì´ˆê¸°ì—ëŠ” ë¹ˆ í‘œë¥¼ ë³´ì—¬ì£¼ê³ , ê·¸ ì•„ë˜ì— ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œ
        empty_df = pd.DataFrame(columns=['keyword', 'recent_freq', 'past_freq', 'surge_ratio'])
        table_placeholder.table(empty_df) # ë¹ˆ í‘œë¥¼ ë¯¸ë¦¬ ë Œë”ë§
        status_message_placeholder.info("ê²€ìƒ‰ ì¡°ê±´ì„ ì…ë ¥í•˜ê³  'ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")


# --- ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
st.header("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
all_db_articles = get_all_articles()

if all_db_articles:
    # content í•„ë“œê°€ Noneì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ CSV/Excel ê¹¨ì§ ë°©ì§€
    df_all_articles = pd.DataFrame(all_db_articles, columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©', 'ìˆ˜ì§‘_ì‹œê°„'])
    df_all_articles['ë‚´ìš©'] = df_all_articles['ë‚´ìš©'].fillna('') # None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ìš°ê¸°
    
    # CSV ë°ì´í„° ìƒì„±
    csv_data = df_all_articles.to_csv(index=False, encoding='utf-8-sig')
    
    # TXT ë°ì´í„° ìƒì„± (ëª¨ë“  ìˆ˜ì§‘ ë‰´ìŠ¤)
    txt_data_lines = []
    for index, row in df_all_articles.iterrows():
        txt_data_lines.append(f"ì œëª©: {row['ì œëª©']}")
        txt_data_lines.append(f"ë§í¬: {row['ë§í¬']}")
        txt_data_lines.append(f"ë‚ ì§œ: {row['ë‚ ì§œ']}")
        txt_data_lines.append(f"ë‚´ìš©: {row['ë‚´ìš©']}")
        txt_data_lines.append(f"ìˆ˜ì§‘ ì‹œê°„: {row['ìˆ˜ì§‘_ì‹œê°„']}")
        txt_data_lines.append("-" * 50) # êµ¬ë¶„ì„ 
    txt_data_all_crawled = "\n".join(txt_data_lines) # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜

    # AI ìš”ì•½ëœ ê¸°ì‚¬ ë°ì´í„° ìƒì„± (final_collected_articles)
    # final_collected_articlesê°€ ë¹„ì–´ìˆì„ ê²½ìš° ë¹ˆ DataFrame ìƒì„±
    df_ai_summaries = pd.DataFrame(st.session_state['final_collected_articles'], 
                                   columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©']) # ì»¬ëŸ¼ ëª…ì‹œ
    df_ai_summaries['ë‚´ìš©'] = df_ai_summaries['ë‚´ìš©'].fillna('') # None ê°’ ì²˜ë¦¬

    # AI ìš”ì•½ TXT ë°ì´í„° ìƒì„± (ìˆ˜ì •ëœ ë¶€ë¶„)
    txt_data_summaries_lines = []
    if not df_ai_summaries.empty:
        for index, row in df_ai_summaries.iterrows():
            txt_data_summaries_lines.append(f"ì œëª©: {row['ì œëª©']}")
            txt_data_summaries_lines.append(f"ë§í¬: {row['ë§í¬']}")
            txt_data_summaries_lines.append(f"ë‚ ì§œ: {row['ë‚ ì§œ']}")
            txt_data_summaries_lines.append(f"ìš”ì•½ ë‚´ìš©: {row['ë‚´ìš©']}")
            txt_data_summaries_lines.append("-" * 50)
    txt_data_ai_summaries = "\n".join(txt_data_summaries_lines) # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜

    # AI ìš”ì•½ XLSX ë°ì´í„° ìƒì„±
    excel_data_ai_summaries = None
    if not df_ai_summaries.empty:
        # BytesIO ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ì—ì„œ Excel íŒŒì¼ ìƒì„±
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_ai_summaries.to_excel(writer, index=False, sheet_name='AI_Summaries')
        excel_data_ai_summaries = output.getvalue()


    st.markdown("### ğŸ“Š ìˆ˜ì§‘ëœ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°")
    col_all_data1, col_all_data2, col_all_data3 = st.columns(3)
    with col_all_data1:
        st.download_button(
            label="ğŸ“„ TXT ë‹¤ìš´ë¡œë“œ",
            data=txt_data_all_crawled,
            file_name=f"all_crawled_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
        )
    with col_all_data2:
        st.download_button(
            label="ğŸ“Š CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name=f"all_crawled_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ì—‘ì…€ì—ì„œ ê¹¨ì§ˆ ê²½ìš° ì•„ë˜ ì•ˆë‚´ ì°¸ì¡°)"
        )
    with col_all_data3:
        # XLSX ë‹¤ìš´ë¡œë“œ (ëª¨ë“  ìˆ˜ì§‘ ë‰´ìŠ¤)
        excel_data_all_crawled = None
        output_all_crawled = BytesIO()
        with pd.ExcelWriter(output_all_crawled, engine='xlsxwriter') as writer:
            df_all_articles.to_excel(writer, index=False, sheet_name='All_Crawled_News')
        excel_data_all_crawled = output_all_crawled.getvalue()

        st.download_button(
            label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_data_all_crawled, # BytesIO.getvalue()ë¡œ ë³€í™˜ëœ ë°ì´í„° ì‚¬ìš©
            file_name=f"all_crawled_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (í•œê¸€ ê¹¨ì§ ì—†ìŒ)"
        )
    
    if not df_ai_summaries.empty:
        st.markdown("### ğŸ“ AI ìš”ì•½ ê¸°ì‚¬")
        col_ai1, col_ai2 = st.columns(2)
        with col_ai1:
            st.download_button(
                label="ğŸ“„ AI ìš”ì•½ TXT ë‹¤ìš´ë¡œë“œ",
                data=txt_data_ai_summaries, # ìˆ˜ì •ëœ ë¶€ë¶„: ë¬¸ìì—´ë¡œ ë³€í™˜ëœ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                file_name=f"ai_summaries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            )
        with col_ai2:
            st.download_button(
                label="ğŸ“Š AI ìš”ì•½ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=excel_data_ai_summaries,
                file_name=f"ai_summaries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            )
    else:
        st.info("AI ìš”ì•½ëœ íŠ¸ë Œë“œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ ìš”ì•½ëœ ê¸°ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.")


    st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
    col_db_info, col_db_clear = st.columns([2, 1])
    with col_db_info:
        st.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {len(all_db_articles)}ê°œì˜ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        # DB ì´ˆê¸°í™” í›„ ë©”ì‹œì§€ í‘œì‹œ
        if st.session_state['db_status_message']:
            if st.session_state['db_status_type'] == "success":
                st.success(st.session_state['db_status_message'])
            elif st.session_state['db_status_type'] == "error":
                st.error(st.session_state['db_status_message'])
            st.session_state['db_status_message'] = "" # ë©”ì‹œì§€ í‘œì‹œ í›„ ì´ˆê¸°í™”
            st.session_state['db_status_type'] = ""
        st.markdown("ğŸ’¡ **CSV íŒŒì¼ì´ ì—‘ì…€ì—ì„œ ê¹¨ì§ˆ ê²½ìš°:** ì—‘ì…€ì—ì„œ 'ë°ì´í„°' íƒ­ -> 'í…ìŠ¤íŠ¸/CSV ê°€ì ¸ì˜¤ê¸°'ë¥¼ í´ë¦­í•œ í›„, 'ì›ë³¸ íŒŒì¼' ì¸ì½”ë”©ì„ 'UTF-8'ë¡œ ì„ íƒí•˜ì—¬ ê°€ì ¸ì˜¤ì„¸ìš”.")
    with col_db_clear:
        if st.button("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”", help="ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì €ì¥ëœ ë‰´ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.", type="secondary"):
            clear_db_content()
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”í•˜ì—¬ í™”ë©´ë„ ë¹„ìš°ê¸°
            st.session_state['trending_keywords_data'] = []
            st.session_state['displayed_trending_keywords'] = []
            st.session_state['final_collected_articles'] = []
            st.session_state['submitted_flag'] = False
            st.session_state['analysis_completed'] = False # ë¶„ì„ ì™„ë£Œ í”Œë˜ê·¸ë„ ì´ˆê¸°í™”
            st.rerun() # DB ì´ˆê¸°í™” í›„ ì•± ì¬ì‹¤í–‰í•˜ì—¬ í™”ë©´ ì—…ë°ì´íŠ¸

