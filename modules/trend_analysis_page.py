# modules/trend_analysis_page.py

import streamlit as st
from datetime import datetime, timedelta
import time
import re
import os
import json
import pandas as pd
from dotenv import load_dotenv # ì´ í˜ì´ì§€ì—ì„œëŠ” os.getenvë¡œ ë°”ë¡œ ì ‘ê·¼í•˜ë¯€ë¡œ load_dotenvëŠ” í•„ìš” ì—†ìŒ
from io import BytesIO

# --- ëª¨ë“ˆ ì„í¬íŠ¸ (ê²½ë¡œ ì¡°ì •) ---
# pages ë””ë ‰í† ë¦¬ì—ì„œ modules ë””ë ‰í† ë¦¬ë¡œ ì ‘ê·¼í•˜ê¸° ìœ„í•´ 'modules.' ì ‘ë‘ì‚¬ ì‚¬ìš©
from modules import ai_service
from modules import database_manager
from modules import news_crawler
from modules import trend_analyzer
from modules import data_exporter


# --- í˜ì´ì§€ í•¨ìˆ˜ ì •ì˜ ---
def trend_analysis_page():
    """
    ìµœì‹  ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±ì„ ìˆ˜í–‰í•˜ëŠ” í˜ì´ì§€ì…ë‹ˆë‹¤.
    """
    # st.set_page_configëŠ” main_app.pyì—ì„œ ì„¤ì •í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
    st.title("ğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
    st.markdown("ì›í•˜ëŠ” í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ê°ì§€í•˜ê³ , AIê°€ ìš”ì•½í•œ ê¸°ì‚¬ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

    # --- ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼ ---
    if st.button("â¬…ï¸ ë©”ì¸ìœ¼ë¡œ"):
        st.session_state.page = "landing"
        st.rerun()
    st.markdown("---") # ë²„íŠ¼ ì•„ë˜ êµ¬ë¶„ì„  ì¶”ê°€

    # --- Potens.dev AI API í‚¤ ì„¤ì • ---
    # main_app.pyì—ì„œ ì´ë¯¸ load_dotenv()ë¥¼ í˜¸ì¶œí–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” os.getenvë¡œ ë°”ë¡œ ì ‘ê·¼
    POTENS_API_KEY = os.getenv("POTENS_API_KEY")

    if not POTENS_API_KEY:
        st.error("ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— 'POTENS_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Potens.dev AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•Šë„ë¡ return
        return

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ main_appì—ì„œ ì´ë¯¸ í˜¸ì¶œë  ìˆ˜ ìˆìœ¼ë‚˜, í˜ì´ì§€ ì§„ì… ì‹œ ì¬í™•ì¸)
    database_manager.init_db()

    # --- Streamlit Session State ì´ˆê¸°í™” (í˜ì´ì§€ ì§„ì… ì‹œ í•„ìš”í•œ ê²½ìš°) ---
    # ê° í˜ì´ì§€ëŠ” ìì‹ ì˜ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ë¥¼ ëª…í™•íˆ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    # main_app.pyì—ì„œ ê³µí†µ ë³€ìˆ˜ëŠ” ì´ˆê¸°í™”í–ˆì§€ë§Œ, í˜ì´ì§€ë³„ ë³€ìˆ˜ëŠ” ì—¬ê¸°ì„œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    if 'trending_keywords_data' not in st.session_state:
        st.session_state['trending_keywords_data'] = []
    if 'displayed_keywords' not in st.session_state:
        st.session_state['displayed_keywords'] = []
    if 'final_collected_articles' not in st.session_state:
        st.session_state['final_collected_articles'] = []
    if 'ai_insights_summary' not in st.session_state:
        st.session_state['ai_insights_summary'] = ""
    if 'ai_trend_summary' not in st.session_state:
        st.session_state['ai_trend_summary'] = ""
    if 'ai_insurance_info' not in st.session_state:
        st.session_state['ai_insurance_info'] = ""


    # --- UI ë ˆì´ì•„ì›ƒ: ê²€ìƒ‰ ì¡°ê±´ (ì¢Œ) & í‚¤ì›Œë“œ íŠ¸ë Œë“œ ê²°ê³¼ (ìš°) ---
    col_search_input, col_trend_results = st.columns([1, 2])

    with col_search_input:
        st.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •")
        with st.form("search_form"):
            keyword = st.text_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ í‚¤ì›Œë“œ (ì˜ˆ: 'ì „ê¸°ì°¨')", value="ì „ê¸°ì°¨", key="keyword_input")
            total_search_days = st.number_input("ì´ ëª‡ ì¼ê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ê¹Œìš”? (ì˜ˆ: 15)", min_value=1, value=15, key="total_days_input")
            recent_trend_days = st.number_input("ìµœê·¼ ëª‡ ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í• ê¹Œìš”? (ì˜ˆ: 2)", min_value=1, value=2, key="recent_days_input")
            max_naver_search_pages_per_day = st.number_input("ê° ë‚ ì§œë³„ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ëª‡ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§í• ê¹Œìš”? (í˜ì´ì§€ë‹¹ 10ê°œ ê¸°ì‚¬, ì˜ˆ: 3)", min_value=1, value=3, key="max_pages_input")

            submitted = st.form_submit_button("ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")

    with col_trend_results:
        st.header("ğŸ“ˆ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼")
        st.markdown("ë‹¤ìŒì€ ìµœê·¼ ì–¸ê¸‰ëŸ‰ì´ ê¸‰ì¦í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")

        table_placeholder = st.empty()
        status_message_placeholder = st.empty()

        if submitted:
            # ìƒˆë¡œìš´ ê²€ìƒ‰ ìš”ì²­ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state['trending_keywords_data'] = []
            st.session_state['displayed_keywords'] = []
            st.session_state['final_collected_articles'] = []
            st.session_state['ai_insights_summary'] = ""
            st.session_state['ai_trend_summary'] = ""
            st.session_state['ai_insurance_info'] = ""

            st.session_state['submitted_flag'] = True
            st.session_state['analysis_completed'] = False
            st.session_state['db_status_message'] = "" # ì œì¶œ ì‹œì—ë„ ì´ˆê¸°í™”
            st.session_state['db_status_type'] = ""     # ì œì¶œ ì‹œì—ë„ ì´ˆê¸°í™”

            table_placeholder.empty()
            my_bar = status_message_placeholder.progress(0, text="ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì§„í–‰ ì¤‘...")
            status_message_placeholder.info("ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

            if recent_trend_days >= total_search_days:
                status_message_placeholder.error("ì˜¤ë¥˜: ìµœê·¼ íŠ¸ë Œë“œ ë¶„ì„ ê¸°ê°„ì€ ì´ ê²€ìƒ‰ ê¸°ê°„ë³´ë‹¤ ì§§ì•„ì•¼ í•©ë‹ˆë‹¤.")
            else:
                all_collected_news_metadata = []

                today_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                search_start_date = today_date - timedelta(days=total_search_days - 1)

                total_expected_articles = total_search_days * max_naver_search_pages_per_day * 10
                processed_article_count = 0


                for i in range(total_search_days):
                    current_search_date = search_start_date + timedelta(days=i)
                    formatted_search_date = current_search_date.strftime('%Y.%m.%d')

                    daily_articles = news_crawler.crawl_naver_news_metadata(
                        keyword,
                        current_search_date,
                        max_naver_search_pages_per_day
                    )

                    for article in daily_articles:
                        processed_article_count += 1
                        progress_percentage = processed_article_count / total_expected_articles
                        my_bar.progress(min(progress_percentage, 1.0), text=f"ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({formatted_search_date}, {processed_article_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ)")


                        article_data_for_db = {
                            "ì œëª©": article["ì œëª©"],
                            "ë§í¬": article["ë§í¬"],
                            "ë‚ ì§œ": article["ë‚ ì§œ"].strftime('%Y-%m-%d'),
                            "ë‚´ìš©": article["ë‚´ìš©"]
                        }
                        database_manager.insert_article(article_data_for_db)

                        all_collected_news_metadata.append(article)

                my_bar.empty()
                status_message_placeholder.success(f"ì´ {len(all_collected_news_metadata)}ê°œì˜ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

                # --- 2. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ---
                status_message_placeholder.info("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
                with st.spinner("í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                    trending_keywords_data = trend_analyzer.analyze_keyword_trends(
                        all_collected_news_metadata,
                        recent_days_period=recent_trend_days,
                        total_days_period=total_search_days
                    )
                st.session_state['trending_keywords_data'] = trending_keywords_data

                if trending_keywords_data:
                    # --- AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ì„ ë³„ ---
                    relevant_keywords_from_ai_raw = []
                    with st.spinner("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„ ì¤‘..."):
                        relevant_keywords_from_ai_raw = ai_service.get_relevant_keywords(
                            trending_keywords_data,
                            "ì°¨ëŸ‰ë³´í—˜ì‚¬ì˜ ë³´í—˜ê°œë°œì",
                            POTENS_API_KEY
                        )

                    filtered_trending_keywords = []
                    if relevant_keywords_from_ai_raw:
                        filtered_trending_keywords = [
                            kw_data for kw_data in trending_keywords_data
                            if kw_data['keyword'] in relevant_keywords_from_ai_raw
                        ]
                        filtered_trending_keywords = sorted(filtered_trending_keywords, key=lambda x: x['recent_freq'], reverse=True)

                        status_message_placeholder.info(f"AIê°€ ì„ ë³„í•œ ë³´í—˜ ê°œë°œì ê´€ì ì˜ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ ({len(filtered_trending_keywords)}ê°œ): {[kw['keyword'] for kw in filtered_trending_keywords]}")
                    else:
                        status_message_placeholder.warning("AIê°€ ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                        filtered_trending_keywords = trending_keywords_data

                    top_3_relevant_keywords = filtered_trending_keywords[:3]
                    st.session_state['displayed_keywords'] = top_3_relevant_keywords

                    if top_3_relevant_keywords:
                        pass
                    else:
                        status_message_placeholder.info("ë³´í—˜ ê°œë°œì ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


                    # --- 3. íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ (Potens.dev AI í™œìš©) ---
                    status_message_placeholder.info("íŠ¸ë Œë“œ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ ì¤‘ (Potens.dev AI í˜¸ì¶œ)...")

                    recent_trending_articles_candidates = [
                        article for article in all_collected_news_metadata
                        if article.get("ë‚ ì§œ") and today_date - timedelta(days=recent_trend_days) <= article["ë‚ ì§œ"]
                    ]

                    processed_links = set()

                    articles_for_ai_summary = []
                    for article in recent_trending_articles_candidates:
                        text_for_trend_check = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "")
                        article_keywords_for_trend = trend_analyzer.extract_keywords_from_text(text_for_trend_check)

                        if any(trend_kw['keyword'] in article_keywords_for_trend for trend_kw in top_3_relevant_keywords):
                            articles_for_ai_summary.append(article)

                    total_ai_articles_to_process = len(articles_for_ai_summary)

                    if total_ai_articles_to_process == 0:
                        status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ìµœê·¼ ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        ai_progress_bar = st.progress(0, text=f"AIê°€ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ìš”ì•½ ì¤‘... (0/{total_ai_articles_to_process} ì™„ë£Œ)")
                        ai_processed_count = 0

                        temp_collected_articles = []
                        for article in articles_for_ai_summary:
                            if article["ë§í¬"] in processed_links:
                                continue

                            article_date_str = article["ë‚ ì§œ"].strftime('%Y-%m-%d')

                            ai_processed_content = ai_service.get_article_summary(
                                article["ì œëª©"],
                                article["ë§í¬"],
                                article_date_str,
                                article["ë‚´ìš©"],
                                POTENS_API_KEY,
                                max_attempts=2
                            )

                            final_content = ""
                            if ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                               ai_processed_content.startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."):
                                final_content = f"ë³¸ë¬¸ ìš”ì•½ ì‹¤íŒ¨ (AI ì˜¤ë¥˜): {ai_processed_content}"
                                status_message_placeholder.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {final_content}")
                            else:
                                final_content = ai_service.clean_ai_response_text(ai_processed_content)

                            temp_collected_articles.append({
                                "ì œëª©": article["ì œëª©"],
                                "ë§í¬": article["ë§í¬"],
                                "ë‚ ì§œ": article_date_str,
                                "ë‚´ìš©": final_content
                            })
                            processed_links.add(article["ë§í¬"])
                            time.sleep(0.1)

                        ai_progress_bar.empty()
                        st.session_state['final_collected_articles'] = temp_collected_articles

                        if st.session_state['final_collected_articles']:
                            status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                            # --- 4. AIê°€ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (ë¶„ë¦¬ëœ í˜¸ì¶œ) ---
                            status_message_placeholder.info("AIê°€ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œ ì¤‘ (ë¶„ë¦¬ëœ í˜¸ì¶œ)...")

                            # AIì—ê²Œ ì „ë‹¬í•  ìš”ì•½ëœ ê¸°ì‚¬ ëª©ë¡ (ì „ì²´ ê¸°ì‚¬ ìš”ì•½ë³¸ì„ ì „ë‹¬)
                            articles_for_ai_insight_generation = st.session_state['final_collected_articles'] # ëª¨ë“  ìš”ì•½ ê¸°ì‚¬ ì „ë‹¬

                            # íŠ¸ë Œë“œ ìš”ì•½ í˜¸ì¶œ
                            with st.spinner("AIê°€ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ìš”ì•½ ì¤‘..."):
                                trend_summary = ai_service.get_overall_trend_summary(
                                    articles_for_ai_insight_generation, # ì „ì²´ ìš”ì•½ ê¸°ì‚¬ ëª©ë¡ ì „ë‹¬
                                    POTENS_API_KEY
                                )
                                st.session_state['ai_trend_summary'] = ai_service.clean_ai_response_text(trend_summary)
                                if st.session_state['ai_trend_summary'].startswith("ìš”ì•½ëœ ê¸°ì‚¬ê°€ ì—†ì–´") or \
                                   st.session_state['ai_trend_summary'].startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                                   st.session_state['ai_trend_summary'].startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."):
                                    status_message_placeholder.error(f"AI íŠ¸ë Œë“œ ìš”ì•½ ì‹¤íŒ¨: {st.session_state['ai_trend_summary']}")
                                else:
                                    status_message_placeholder.success("AI ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½ ì™„ë£Œ!")
                                time.sleep(1) # ë‹¤ìŒ UI ì—…ë°ì´íŠ¸ ì „ ì ì‹œ ëŒ€ê¸°

                            # ë³´í—˜ ê´€ë ¨ ì •ë³´ í˜¸ì¶œ
                            with st.spinner("AIê°€ ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘..."):
                                insurance_info = ai_service.get_insurance_implications_from_ai(
                                    st.session_state['ai_trend_summary'], # ë³€ê²½ëœ ë¶€ë¶„: íŠ¸ë Œë“œ ìš”ì•½ë¬¸ ì „ë‹¬
                                    POTENS_API_KEY
                                )
                                st.session_state['ai_insurance_info'] = ai_service.clean_ai_response_text(insurance_info)
                                if st.session_state['ai_insurance_info'].startswith("ìš”ì•½ëœ ê¸°ì‚¬ê°€ ì—†ì–´") or \
                                   st.session_state['ai_insurance_info'].startswith("Potens.dev AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") or \
                                   st.session_state['ai_insurance_info'].startswith("Potens.dev AI í˜¸ì¶œì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") or \
                                   st.session_state['ai_insurance_info'].startswith("íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ì–´"): # íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ëŠ” ê²½ìš°ë„ ì‹¤íŒ¨
                                    status_message_placeholder.error(f"AI ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ ë¶„ì„ ì‹¤íŒ¨: {st.session_state['ai_insurance_info']}")
                                else:
                                    status_message_placeholder.success("AI ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì •ë³´ ë¶„ì„ ì™„ë£Œ!")
                                time.sleep(1) # ë‹¤ìŒ UI ì—…ë°ì´íŠ¸ ì „ ì ì‹œ ëŒ€ê¸°

                            # ë‘ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±
                            final_insights_text = ""
                            if st.session_state['ai_trend_summary'] and \
                               not st.session_state['ai_trend_summary'].startswith("AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨"):
                                final_insights_text += "### ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½\n"
                                final_insights_text += st.session_state['ai_trend_summary'] + "\n\n"
                            else:
                                final_insights_text += "### ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½ (ìƒì„± ì‹¤íŒ¨)\n"
                                final_insights_text += st.session_state['ai_trend_summary'] + "\n\n"

                            if st.session_state['ai_insurance_info'] and \
                               not st.session_state['ai_insurance_info'].startswith("AI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨") and \
                               not st.session_state['ai_insurance_info'].startswith("íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ì–´"): # íŠ¸ë Œë“œ ìš”ì•½ë¬¸ì´ ì—†ëŠ” ê²½ìš°ë„ ì‹¤íŒ¨
                                final_insights_text += "### ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì£¼ìš” ì‚¬ì‹¤ ë° ë²•ì  ì±…ì„\n"
                                final_insights_text += st.session_state['ai_insurance_info'] + "\n"
                            else:
                                final_insights_text += "### ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì£¼ìš” ì‚¬ì‹¤ ë° ë²•ì  ì±…ì„ (ìƒì„± ì‹¤íŒ¨)\n"
                                final_insights_text += st.session_state['ai_insurance_info'] + "\n"

                            # --- ë¶€ë¡ ì„¹ì…˜ ì¶”ê°€ ---
                            final_insights_text += "\n---\n\n"
                            final_insights_text += "## ë¶€ë¡\n\n"

                            # í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±° ì¶”ê°€
                            final_insights_text += "### í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±°\n"
                            if st.session_state['displayed_keywords']:
                                for kw_data in st.session_state['displayed_keywords']:
                                    # f-string ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •
                                    surge_ratio_display = (f'''{kw_data.get('surge_ratio'):.2f}x''' if kw_data.get('surge_ratio') != float('inf') else 'ìƒˆë¡œìš´ íŠ¸ë Œë“œ')
                                    final_insights_text += (
                                        f"- **í‚¤ì›Œë“œ**: {kw_data['keyword']}\n"
                                        f"  - ìµœê·¼ ì–¸ê¸‰ëŸ‰: {kw_data['recent_freq']}íšŒ\n"
                                        f"  - ì´ì „ ì–¸ê¸‰ëŸ‰: {kw_data['past_freq']}íšŒ\n"
                                        f"  - ì¦ê°€ìœ¨: {surge_ratio_display}\n"
                                    )
                            else:
                                final_insights_text += "í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                            final_insights_text += "\n"

                            # ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ (ë°°ì¹˜ ìš”ì•½ ëŒ€ì‹  ì›ë³¸ ê¸°ì‚¬ ì •ë³´ ë‚˜ì—´)
                            final_insights_text += "### ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸\n"
                            if st.session_state['final_collected_articles']:
                                for i, article in enumerate(st.session_state['final_collected_articles']):
                                    final_insights_text += (
                                        f"{i+1}. **ì œëª©**: {article['ì œëª©']}\n"
                                        f"   **ë‚ ì§œ**: {article['ë‚ ì§œ']}\n" # ì˜¤íƒ€ ìˆ˜ì •
                                        f"   **ë§í¬**: {article['ë§í¬']}\n"
                                        f"   **ìš”ì•½ ë‚´ìš©**: {article['ë‚´ìš©'][:100]}...\n" # ìš”ì•½ ë‚´ìš©ì˜ ì¼ë¶€ë§Œ í‘œì‹œ
                                    )
                                final_insights_text += "\n"
                            else:
                                final_insights_text += "ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

                            st.session_state['ai_insights_summary'] = final_insights_text

                        else:
                            status_message_placeholder.info("ì„ ë³„ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜, AI ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                else:
                    status_message_placeholder.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

            st.session_state['submitted_flag'] = False
            st.session_state['analysis_completed'] = True

        # --- ê²°ê³¼ê°€ ì´ë¯¸ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ” ê²½ìš° í‘œì‹œ ---
        if not st.session_state.get('submitted_flag', False) and \
           st.session_state.get('analysis_completed', False):
            if st.session_state['displayed_keywords']:
                df_top_keywords = pd.DataFrame(st.session_state['displayed_keywords'])
                df_top_keywords['surge_ratio'] = df_top_keywords['surge_ratio'].apply(
                    lambda x: f"{x:.2f}x" if x != float('inf') else "ìƒˆë¡œìš´ íŠ¸ë Œë“œ"
                )
                table_placeholder.table(df_top_keywords)

                if st.session_state['final_collected_articles']:
                    status_message_placeholder.success(f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                    # AI ì¸ì‚¬ì´íŠ¸ ìš”ì•½ í‘œì‹œ (íŠ¸ë Œë“œ ë³´ê³ ì„œ)
                    # ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ í™”ë©´ì— í‘œì‹œë˜ì§€ ì•Šë„ë¡ í•¨
                    # if st.session_state['ai_insights_summary']:
                    #     st.markdown("---")
                    #     st.subheader("ğŸ’¡ AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸")
                    #     st.markdown(st.session_state['ai_insights_summary'])
                    # else:
                    #     st.info("AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                    
                    # ëŒ€ì‹ , ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ì— ë‹¤ìš´ë¡œë“œ ì•ˆë‚´ ì¶”ê°€
                    status_message_placeholder.success(
                        f"ì´ {len(st.session_state['final_collected_articles'])}ê°œì˜ íŠ¸ë Œë“œ ê¸°ì‚¬ ìš”ì•½ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. "
                        "AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œëŠ” ì•„ë˜ 'ë°ì´í„° ë‹¤ìš´ë¡œë“œ' ì„¹ì…˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )


            else:
                st.info("ì„ íƒëœ ê¸°ê°„ ë‚´ì— ìœ ì˜ë¯¸í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # --- ì´ˆê¸° ë¡œë“œ ì‹œ ë©”ì‹œì§€ ---
        elif not st.session_state.get('submitted_flag', False) and \
             not st.session_state.get('analysis_completed', False):
            empty_df = pd.DataFrame(columns=['keyword', 'recent_freq', 'past_freq', 'surge_ratio'])
            table_placeholder.table(empty_df)
            status_message_placeholder.info("ê²€ìƒ‰ ì¡°ê±´ì„ ì…ë ¥í•˜ê³  'ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")


    # --- ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
    st.header("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    all_db_articles = database_manager.get_all_articles()

    if all_db_articles:
        # ë³€ìˆ˜ ì´ˆê¸°í™”
        txt_data_all_crawled = ""
        excel_data_all_crawled = None
        txt_data_ai_summaries = ""
        excel_data_ai_summaries = None
        txt_data_ai_insights = ""
        excel_data_ai_insights = None

        df_all_articles = pd.DataFrame(all_db_articles, columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©', 'ìˆ˜ì§‘_ì‹œê°„'])
        df_all_articles['ë‚´ìš©'] = df_all_articles['ë‚´ìš©'].fillna('')

        txt_data_all_crawled = data_exporter.export_articles_to_txt(
            [dict(zip(df_all_articles.columns, row)) for row in df_all_articles.values],
            file_prefix="all_crawled_news"
        )

        excel_data_all_crawled = data_exporter.export_articles_to_excel(df_all_articles, sheet_name='All_Crawled_News')


        df_ai_summaries = pd.DataFrame(st.session_state['final_collected_articles'],
                                       columns=['ì œëª©', 'ë§í¬', 'ë‚ ì§œ', 'ë‚´ìš©'])
        df_ai_summaries['ë‚´ìš©'] = df_ai_summaries['ë‚´ìš©'].fillna('')

        txt_data_ai_summaries = data_exporter.export_articles_to_txt(
            [dict(zip(df_ai_summaries.columns, row)) for row in df_ai_summaries.values],
            file_prefix="ai_summaries"
        )

        if not df_ai_summaries.empty:
            excel_data_ai_summaries = data_exporter.export_articles_to_excel(df_ai_summaries, sheet_name='AI_Summaries')

        txt_data_ai_insights = st.session_state['ai_insights_summary']

        if st.session_state['ai_insights_summary']:
            ai_insights_df = pd.DataFrame({
                'ë³´ê³ ì„œ ì„¹ì…˜': ['ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½', 'ìë™ì°¨ ë³´í—˜ ì‚°ì—… ê´€ë ¨ ì£¼ìš” ì‚¬ì‹¤ ë° ë²•ì  ì±…ì„', 'í‚¤ì›Œë“œ ì‚°ì¶œ ê·¼ê±°', 'ë°˜ì˜ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸'],
                'ë‚´ìš©': [
                    st.session_state['ai_trend_summary'],
                    st.session_state['ai_insurance_info'],
                    "\n".join([f"- {kw_data['keyword']}: ìµœê·¼ {kw_data['recent_freq']}íšŒ, ì´ì „ {kw_data['past_freq']}íšŒ, ì¦ê°€ìœ¨ {f'''{kw_data.get('surge_ratio'):.2f}x''' if kw_data.get('surge_ratio') != float('inf') else 'ìƒˆë¡œìš´ íŠ¸ë Œë“œ'}" for kw_data in st.session_state['displayed_keywords']]),
                    "\n".join([f"{i+1}. ì œëª©: {art['ì œëª©']}\në‚ ì§œ: {art['ë‚ ì§œ']}\në§í¬: {art['ë§í¬']}\nìš”ì•½ ë‚´ìš©: {art['ë‚´ìš©'][:100]}..." for i, art in enumerate(st.session_state['final_collected_articles'])])
                ]
            })
            excel_data_ai_insights = data_exporter.export_articles_to_excel(ai_insights_df, sheet_name='AI_Insights_Report')


        st.markdown("### ğŸ“Š ìˆ˜ì§‘ëœ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°")
        col_all_data_txt, col_all_data_excel = st.columns([0.1, 0.9])
        with col_all_data_txt:
            st.download_button(
                label="ğŸ“„ TXT ë‹¤ìš´ë¡œë“œ",
                data=txt_data_all_crawled,
                file_name=data_exporter.generate_filename("all_crawled_news", "txt"),
                mime="text/plain",
                help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            )
        with col_all_data_excel:
            if excel_data_all_crawled:
                st.download_button(
                    label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=excel_data_all_crawled.getvalue(),
                    file_name=data_exporter.generate_filename("all_crawled_news", "xlsx"),
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (í•œê¸€ ê¹¨ì§ ì—†ìŒ)"
                )
            else:
                st.info("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


        if not df_ai_summaries.empty:
            st.markdown("### ğŸ“ AI ìš”ì•½ ê¸°ì‚¬")
            col_ai_txt, col_ai_excel = st.columns([0.1, 0.9])
            with col_ai_txt:
                st.download_button(
                    label="ğŸ“„ AI ìš”ì•½ TXT ë‹¤ìš´ë¡œë“œ",
                    data=txt_data_ai_summaries,
                    file_name=data_exporter.generate_filename("ai_summaries", "txt"),
                    mime="text/plain",
                    help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                )
            with col_ai_excel:
                if excel_data_ai_summaries:
                    st.download_button(
                        label="ğŸ“Š AI ìš”ì•½ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                        data=excel_data_ai_summaries.getvalue(),
                        file_name=data_exporter.generate_filename("ai_summaries", "xlsx"),
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        help="AIê°€ ìš”ì•½í•œ íŠ¸ë Œë“œ ê¸°ì‚¬ ë‚´ìš©ì„ ì—‘ì…€ íŒŒì¼(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                    )
                else:
                    st.info("AI ìš”ì•½ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("AI ìš”ì•½ëœ íŠ¸ë Œë“œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ ìš”ì•½ëœ ê¸°ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

        if st.session_state['ai_insights_summary']:
            st.markdown("### ğŸ’¡ AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸")
            col_ai_insights_txt, col_ai_insights_excel = st.columns([0.1, 0.9])
            with col_ai_insights_txt:
                st.download_button(
                    label="ğŸ“„ TXT ë‹¤ìš´ë¡œë“œ",
                    data=txt_data_ai_insights,
                    file_name=data_exporter.generate_filename("ai_insights_report", "txt"),
                    mime="text/plain",
                    help="AIê°€ ë„ì¶œí•œ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                )
            with col_ai_insights_excel:
                if excel_data_ai_insights:
                    st.download_button(
                        label="ğŸ“Š ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                        data=excel_data_ai_insights.getvalue(),
                        file_name=data_exporter.generate_filename("ai_insights_report", "xlsx"),
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        help="AIê°€ ë„ì¶œí•œ íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                    )
                else:
                    st.info("AI ì¸ì‚¬ì´íŠ¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("AI íŠ¸ë Œë“œ ìš”ì•½ ë° ë³´í—˜ ìƒí’ˆ ê°œë°œ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”.")


        st.markdown("---")
        col_db_info, col_db_clear = st.columns([2, 1])
        with col_db_info:
            st.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {len(all_db_articles)}ê°œì˜ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            if st.session_state['db_status_message']:
                if st.session_state['db_status_type'] == "success":
                    st.success(st.session_state['db_status_message'])
                elif st.session_state['db_status_type'] == "error":
                    st.error(st.session_state['db_status_message'])
                st.session_state['db_status_message'] = ""
                st.session_state['db_status_type'] = ""
            st.markdown("ğŸ’¡ **CSV íŒŒì¼ì´ ì—‘ì…€ì—ì„œ ê¹¨ì§ˆ ê²½ìš°:** ì—‘ì…€ì—ì„œ 'ë°ì´í„°' íƒ­ -> 'í…ìŠ¤íŠ¸/CSV ê°€ì ¸ì˜¤ê¸°'ë¥¼ í´ë¦­í•œ í›„, 'ì›ë³¸ íŒŒì¼' ì¸ì½”ë”©ì„ 'UTF-8'ë¡œ ì„ íƒí•˜ì—¬ ê°€ì ¸ì˜¤ì„¸ìš”.")
        with col_db_clear:
            if st.button("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”", help="ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì €ì¥ëœ ë‰´ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.", type="secondary"):
                database_manager.clear_db_content()
                st.session_state['trending_keywords_data'] = []
                st.session_state['displayed_keywords'] = []
                st.session_state['final_collected_articles'] = []
                st.session_state['ai_insights_summary'] = ""
                st.session_state['ai_trend_summary'] = ""
                st.session_state['ai_insurance_info'] = ""
                st.session_state['submitted_flag'] = False
                st.session_state['analysis_completed'] = False
                st.rerun()
